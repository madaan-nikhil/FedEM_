{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Femnist Table 1\n",
    "\n",
    "TJ Kim\n",
    "\n",
    "1.17.22\n",
    "\n",
    "#### Summary:\n",
    "- Make a table for Benign transferability and inter-boundary distance for following models\n",
    "- Local benign, fedavg benign, fedEM benign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/FedEM\n"
     ]
    }
   ],
   "source": [
    "cd /home/ubuntu/FedEM/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import General Libraries\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import copy\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import FedEM based Libraries\n",
    "from utils.utils import *\n",
    "from utils.constants import *\n",
    "from utils.args import *\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from run_experiment import *\n",
    "from models import *\n",
    "\n",
    "# Import Transfer Attack\n",
    "from transfer_attacks.Personalized_NN import *\n",
    "from transfer_attacks.Params import *\n",
    "from transfer_attacks.Transferer import *\n",
    "from transfer_attacks.Args import *\n",
    "\n",
    "from transfer_attacks.TA_utils import *\n",
    "from transfer_attacks.Boundary_Transferer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local Benign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Clients initialization..\n",
      "===> Building data iterators..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 719/719 [00:01<00:00, 666.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Initializing clients..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 719/719 [00:06<00:00, 111.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Test Clients initialization..\n",
      "===> Building data iterators..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Initializing clients..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++\n",
      "Global..\n",
      "Train Loss: 4.147 | Train Acc: 1.769% |Test Loss: 4.146 | Test Acc: 1.699% |\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "################################################################################\n"
     ]
    }
   ],
   "source": [
    "setting = 'local'\n",
    "\n",
    "if setting == 'FedEM':\n",
    "    nL = 3\n",
    "else:\n",
    "    nL = 1\n",
    "\n",
    "# Manually set argument parameters\n",
    "args_ = Args()\n",
    "args_.experiment = \"femnist\"\n",
    "args_.method = setting\n",
    "args_.decentralized = False\n",
    "args_.sampling_rate = 1.0\n",
    "args_.input_dimension = None\n",
    "args_.output_dimension = None\n",
    "args_.n_learners= nL\n",
    "args_.n_rounds = 10\n",
    "args_.bz = 128\n",
    "args_.local_steps = 1\n",
    "args_.lr_lambda = 0\n",
    "args_.lr =0.03\n",
    "args_.lr_scheduler = 'multi_step'\n",
    "args_.log_freq = 10\n",
    "args_.device = 'cuda'\n",
    "args_.optimizer = 'sgd'\n",
    "args_.mu = 0\n",
    "args_.communication_probability = 0.1\n",
    "args_.q = 1\n",
    "args_.locally_tune_clients = False\n",
    "args_.seed = 1234\n",
    "args_.verbose = 1\n",
    "args_.save_path = 'weights/cifar/dummy/'\n",
    "args_.validation = False\n",
    "\n",
    "# Generate the dummy values here\n",
    "aggregator, clients = dummy_aggregator(args_, num_user=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Dataset to be used throughout all analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Validation Data across all clients as test\n",
    "data_x = []\n",
    "data_y = []\n",
    "\n",
    "for i in range(len(clients)):\n",
    "    daniloader = clients[i].test_iterator\n",
    "    for (x,y,idx) in daniloader.dataset:\n",
    "        data_x.append(x)\n",
    "        data_y.append(y)\n",
    "\n",
    "data_x = torch.stack(data_x)\n",
    "try:\n",
    "    data_y = torch.stack(data_y)        \n",
    "except:\n",
    "    data_y = torch.FloatTensor(data_y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = Custom_Dataloader(data_x, data_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load local model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_models = 40\n",
    "\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.2f}\".format(x)})\n",
    "\n",
    "if setting == 'local':\n",
    "\n",
    "#     args_.save_path = 'weights/final/femnist/fig1_take3/local_benign/'\n",
    "    args_.save_path ='weights/final/femnist/fig1_take3/local_adv/'\n",
    "    aggregator.load_state(args_.save_path)\n",
    "    \n",
    "    model_weights = []\n",
    "#     weights = np.load(\"weights/final/femnist/fig1_take3/local_benign/train_client_weights.npy\")\n",
    "    weights = np.load('weights/final/femnist/fig1_take3/local_adv/train_client_weights.npy')\n",
    "    \n",
    "    for i in range(num_models):\n",
    "        model_weights += [weights[i]]\n",
    "\n",
    "    # Generate the weights to test on as linear combinations of the model_weights\n",
    "    models_test = []\n",
    "\n",
    "    for i in range(num_models):\n",
    "        new_model = copy.deepcopy(aggregator.clients[i].learners_ensemble.learners[0].model)\n",
    "        new_model.eval()\n",
    "        models_test += [new_model]\n",
    "\n",
    "elif setting == 'FedAvg':\n",
    "    \n",
    "#     args_.save_path = 'weights/final/femnist/fig1_take3/fedavg_benign/'\n",
    "    args_.save_path = 'weights/final/femnist/fig1_take3/FedAvg_adv/'\n",
    "    aggregator.load_state(args_.save_path)\n",
    "    \n",
    "    # This is where the models are stored -- one for each mixture --> learner.model for nn\n",
    "    hypotheses = aggregator.global_learners_ensemble.learners\n",
    "\n",
    "    # obtain the state dict for each of the weights \n",
    "    weights_h = []\n",
    "\n",
    "    for h in hypotheses:\n",
    "        weights_h += [h.model.state_dict()]\n",
    "\n",
    "#     weights = np.load(\"weights/final/femnist/fig1_take3/fedavg_benign/train_client_weights.npy\")\n",
    "    weights = np.load('weights/final/femnist/fig1_take3/FedAvg_adv/train_client_weights.npy')\n",
    "    \n",
    "    # Set model weights\n",
    "    model_weights = []\n",
    "\n",
    "    for i in range(num_models):\n",
    "        model_weights += [weights[i]]\n",
    "\n",
    "    # Generate the weights to test on as linear combinations of the model_weights\n",
    "    models_test = []\n",
    "\n",
    "    for (w0) in model_weights:\n",
    "        # first make the model with empty weights\n",
    "        new_model = copy.deepcopy(hypotheses[0].model)\n",
    "        new_model.eval()\n",
    "        new_weight_dict = copy.deepcopy(weights_h[0])\n",
    "        for key in weights_h[0]:\n",
    "            new_weight_dict[key] = w0[0]*weights_h[0][key] \n",
    "        new_model.load_state_dict(new_weight_dict)\n",
    "        models_test += [new_model]\n",
    "\n",
    "elif setting == 'FedEM':\n",
    "    \n",
    "#     args_.save_path = 'weights/final/femnist/fig1_take3/fedem_benign/'\n",
    "#     args_.save_path = 'weights/final/femnist/fig1_take3/fedem_adv/'\n",
    "    args_.save_path = 'weights/final/femnist/figperturb/fedem_avg_p0_1/'\n",
    "    aggregator.load_state(args_.save_path)\n",
    "    \n",
    "    # This is where the models are stored -- one for each mixture --> learner.model for nn\n",
    "    hypotheses = aggregator.global_learners_ensemble.learners\n",
    "\n",
    "    # obtain the state dict for each of the weights \n",
    "    weights_h = []\n",
    "\n",
    "    for h in hypotheses:\n",
    "        weights_h += [h.model.state_dict()]\n",
    "\n",
    "#     weights = np.load(\"weights/final/femnist/fig1_take3/fedem_benign/train_client_weights.npy\")\n",
    "#     weights = np.load(\"weights/final/femnist/fig1_take3/fedem_adv/train_client_weights.npy\")\n",
    "    weights = np.load(\"weights/final/femnist/figperturb/fedem_avg_p0_1/train_client_weights.npy\")\n",
    "\n",
    "    # Set model weights\n",
    "    model_weights = []\n",
    "\n",
    "    for i in range(num_models):\n",
    "        model_weights += [weights[i]]\n",
    "\n",
    "\n",
    "    # Generate the weights to test on as linear combinations of the model_weights\n",
    "    models_test = []\n",
    "\n",
    "    for (w0,w1,w2) in model_weights:\n",
    "        # first make the model with empty weights\n",
    "        new_model = copy.deepcopy(hypotheses[0].model)\n",
    "        new_model.eval()\n",
    "        new_weight_dict = copy.deepcopy(weights_h[0])\n",
    "        for key in weights_h[0]:\n",
    "            new_weight_dict[key] = w0*weights_h[0][key] + w1*weights_h[1][key] + w2*weights_h[2][key]\n",
    "        new_model.load_state_dict(new_weight_dict)\n",
    "        models_test += [new_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [1.00]\n",
      "1 [1.00]\n",
      "2 [1.00]\n",
      "3 [1.00]\n",
      "4 [1.00]\n",
      "5 [1.00]\n",
      "6 [1.00]\n",
      "7 [1.00]\n",
      "8 [1.00]\n",
      "9 [1.00]\n",
      "10 [1.00]\n",
      "11 [1.00]\n",
      "12 [1.00]\n",
      "13 [1.00]\n",
      "14 [1.00]\n",
      "15 [1.00]\n",
      "16 [1.00]\n",
      "17 [1.00]\n",
      "18 [1.00]\n",
      "19 [1.00]\n",
      "20 [1.00]\n",
      "21 [1.00]\n",
      "22 [1.00]\n",
      "23 [1.00]\n",
      "24 [1.00]\n",
      "25 [1.00]\n",
      "26 [1.00]\n",
      "27 [1.00]\n",
      "28 [1.00]\n",
      "29 [1.00]\n",
      "30 [1.00]\n",
      "31 [1.00]\n",
      "32 [1.00]\n",
      "33 [1.00]\n",
      "34 [1.00]\n",
      "35 [1.00]\n",
      "36 [1.00]\n",
      "37 [1.00]\n",
      "38 [1.00]\n",
      "39 [1.00]\n",
      "40 [1.00]\n",
      "41 [1.00]\n",
      "42 [1.00]\n",
      "43 [1.00]\n",
      "44 [1.00]\n",
      "45 [1.00]\n",
      "46 [1.00]\n",
      "47 [1.00]\n",
      "48 [1.00]\n",
      "49 [1.00]\n",
      "50 [1.00]\n",
      "51 [1.00]\n",
      "52 [1.00]\n",
      "53 [1.00]\n",
      "54 [1.00]\n",
      "55 [1.00]\n",
      "56 [1.00]\n",
      "57 [1.00]\n",
      "58 [1.00]\n",
      "59 [1.00]\n",
      "60 [1.00]\n",
      "61 [1.00]\n",
      "62 [1.00]\n",
      "63 [1.00]\n",
      "64 [1.00]\n",
      "65 [1.00]\n",
      "66 [1.00]\n",
      "67 [1.00]\n",
      "68 [1.00]\n",
      "69 [1.00]\n",
      "70 [1.00]\n",
      "71 [1.00]\n",
      "72 [1.00]\n",
      "73 [1.00]\n",
      "74 [1.00]\n",
      "75 [1.00]\n",
      "76 [1.00]\n",
      "77 [1.00]\n",
      "78 [1.00]\n",
      "79 [1.00]\n",
      "80 [1.00]\n",
      "81 [1.00]\n",
      "82 [1.00]\n",
      "83 [1.00]\n",
      "84 [1.00]\n",
      "85 [1.00]\n",
      "86 [1.00]\n",
      "87 [1.00]\n",
      "88 [1.00]\n",
      "89 [1.00]\n",
      "90 [1.00]\n",
      "91 [1.00]\n",
      "92 [1.00]\n",
      "93 [1.00]\n",
      "94 [1.00]\n",
      "95 [1.00]\n",
      "96 [1.00]\n",
      "97 [1.00]\n",
      "98 [1.00]\n",
      "99 [1.00]\n",
      "100 [1.00]\n",
      "101 [1.00]\n",
      "102 [1.00]\n",
      "103 [1.00]\n",
      "104 [1.00]\n",
      "105 [1.00]\n",
      "106 [1.00]\n",
      "107 [1.00]\n",
      "108 [1.00]\n",
      "109 [1.00]\n",
      "110 [1.00]\n",
      "111 [1.00]\n",
      "112 [1.00]\n",
      "113 [1.00]\n",
      "114 [1.00]\n",
      "115 [1.00]\n",
      "116 [1.00]\n",
      "117 [1.00]\n",
      "118 [1.00]\n",
      "119 [1.00]\n",
      "120 [1.00]\n",
      "121 [1.00]\n",
      "122 [1.00]\n",
      "123 [1.00]\n",
      "124 [1.00]\n",
      "125 [1.00]\n",
      "126 [1.00]\n",
      "127 [1.00]\n",
      "128 [1.00]\n",
      "129 [1.00]\n",
      "130 [1.00]\n",
      "131 [1.00]\n",
      "132 [1.00]\n",
      "133 [1.00]\n",
      "134 [1.00]\n",
      "135 [1.00]\n",
      "136 [1.00]\n",
      "137 [1.00]\n",
      "138 [1.00]\n",
      "139 [1.00]\n",
      "140 [1.00]\n",
      "141 [1.00]\n",
      "142 [1.00]\n",
      "143 [1.00]\n",
      "144 [1.00]\n",
      "145 [1.00]\n",
      "146 [1.00]\n",
      "147 [1.00]\n",
      "148 [1.00]\n",
      "149 [1.00]\n",
      "150 [1.00]\n",
      "151 [1.00]\n",
      "152 [1.00]\n",
      "153 [1.00]\n",
      "154 [1.00]\n",
      "155 [1.00]\n",
      "156 [1.00]\n",
      "157 [1.00]\n",
      "158 [1.00]\n",
      "159 [1.00]\n",
      "160 [1.00]\n",
      "161 [1.00]\n",
      "162 [1.00]\n",
      "163 [1.00]\n",
      "164 [1.00]\n",
      "165 [1.00]\n",
      "166 [1.00]\n",
      "167 [1.00]\n",
      "168 [1.00]\n",
      "169 [1.00]\n",
      "170 [1.00]\n",
      "171 [1.00]\n",
      "172 [1.00]\n",
      "173 [1.00]\n",
      "174 [1.00]\n",
      "175 [1.00]\n",
      "176 [1.00]\n",
      "177 [1.00]\n",
      "178 [1.00]\n",
      "179 [1.00]\n",
      "180 [1.00]\n",
      "181 [1.00]\n",
      "182 [1.00]\n",
      "183 [1.00]\n",
      "184 [1.00]\n",
      "185 [1.00]\n",
      "186 [1.00]\n",
      "187 [1.00]\n",
      "188 [1.00]\n",
      "189 [1.00]\n",
      "190 [1.00]\n",
      "191 [1.00]\n",
      "192 [1.00]\n",
      "193 [1.00]\n",
      "194 [1.00]\n",
      "195 [1.00]\n",
      "196 [1.00]\n",
      "197 [1.00]\n",
      "198 [1.00]\n",
      "199 [1.00]\n",
      "200 [1.00]\n",
      "201 [1.00]\n",
      "202 [1.00]\n",
      "203 [1.00]\n",
      "204 [1.00]\n",
      "205 [1.00]\n",
      "206 [1.00]\n",
      "207 [1.00]\n",
      "208 [1.00]\n",
      "209 [1.00]\n",
      "210 [1.00]\n",
      "211 [1.00]\n",
      "212 [1.00]\n",
      "213 [1.00]\n",
      "214 [1.00]\n",
      "215 [1.00]\n",
      "216 [1.00]\n",
      "217 [1.00]\n",
      "218 [1.00]\n",
      "219 [1.00]\n",
      "220 [1.00]\n",
      "221 [1.00]\n",
      "222 [1.00]\n",
      "223 [1.00]\n",
      "224 [1.00]\n",
      "225 [1.00]\n",
      "226 [1.00]\n",
      "227 [1.00]\n",
      "228 [1.00]\n",
      "229 [1.00]\n",
      "230 [1.00]\n",
      "231 [1.00]\n",
      "232 [1.00]\n",
      "233 [1.00]\n",
      "234 [1.00]\n",
      "235 [1.00]\n",
      "236 [1.00]\n",
      "237 [1.00]\n",
      "238 [1.00]\n",
      "239 [1.00]\n",
      "240 [1.00]\n",
      "241 [1.00]\n",
      "242 [1.00]\n",
      "243 [1.00]\n",
      "244 [1.00]\n",
      "245 [1.00]\n",
      "246 [1.00]\n",
      "247 [1.00]\n",
      "248 [1.00]\n",
      "249 [1.00]\n",
      "250 [1.00]\n",
      "251 [1.00]\n",
      "252 [1.00]\n",
      "253 [1.00]\n",
      "254 [1.00]\n",
      "255 [1.00]\n",
      "256 [1.00]\n",
      "257 [1.00]\n",
      "258 [1.00]\n",
      "259 [1.00]\n",
      "260 [1.00]\n",
      "261 [1.00]\n",
      "262 [1.00]\n",
      "263 [1.00]\n",
      "264 [1.00]\n",
      "265 [1.00]\n",
      "266 [1.00]\n",
      "267 [1.00]\n",
      "268 [1.00]\n",
      "269 [1.00]\n",
      "270 [1.00]\n",
      "271 [1.00]\n",
      "272 [1.00]\n",
      "273 [1.00]\n",
      "274 [1.00]\n",
      "275 [1.00]\n",
      "276 [1.00]\n",
      "277 [1.00]\n",
      "278 [1.00]\n",
      "279 [1.00]\n",
      "280 [1.00]\n",
      "281 [1.00]\n",
      "282 [1.00]\n",
      "283 [1.00]\n",
      "284 [1.00]\n",
      "285 [1.00]\n",
      "286 [1.00]\n",
      "287 [1.00]\n",
      "288 [1.00]\n",
      "289 [1.00]\n",
      "290 [1.00]\n",
      "291 [1.00]\n",
      "292 [1.00]\n",
      "293 [1.00]\n",
      "294 [1.00]\n",
      "295 [1.00]\n",
      "296 [1.00]\n",
      "297 [1.00]\n",
      "298 [1.00]\n",
      "299 [1.00]\n"
     ]
    }
   ],
   "source": [
    "for i in range(weights.shape[0]):\n",
    "    print(i, weights[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# desired_servers = [3,64,126,162,245,38,61,81,90,119]\n",
    "desired_servers = [265, 24,39,64,109]\n",
    "new_models_test = []\n",
    "\n",
    "for i in range(len(desired_servers)):\n",
    "    new_models_test += [models_test[i]]\n",
    "\n",
    "new_models_test = models_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Up Transfer Attack Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_adv = []\n",
    "\n",
    "for i in range(num_models):\n",
    "    adv_dict = {}\n",
    "    adv_dict['orig_acc_transfers'] = None\n",
    "    adv_dict['orig_similarities'] = None\n",
    "    adv_dict['adv_acc_transfers'] = None\n",
    "    adv_dict['adv_similarities_target'] = None\n",
    "    adv_dict['adv_similarities_untarget'] = None\n",
    "    adv_dict['adv_target'] = None\n",
    "    adv_dict['adv_miss'] = None\n",
    "    adv_dict['metric_alignment'] = None\n",
    "    adv_dict['ib_distance_legit'] = None\n",
    "    adv_dict['ib_distance_adv'] = None\n",
    "\n",
    "    logs_adv += [adv_dict]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Adv idx: 0\n",
      "\t Adv idx: 1\n",
      "\t Adv idx: 2\n",
      "\t Adv idx: 3\n",
      "\t Adv idx: 4\n",
      "\t Adv idx: 5\n",
      "\t Adv idx: 6\n",
      "\t Adv idx: 7\n",
      "\t Adv idx: 8\n",
      "\t Adv idx: 9\n",
      "\t Adv idx: 10\n",
      "\t Adv idx: 11\n",
      "\t Adv idx: 12\n",
      "\t Adv idx: 13\n",
      "\t Adv idx: 14\n",
      "\t Adv idx: 15\n",
      "\t Adv idx: 16\n",
      "\t Adv idx: 17\n",
      "\t Adv idx: 18\n",
      "\t Adv idx: 19\n",
      "\t Adv idx: 20\n",
      "\t Adv idx: 21\n",
      "\t Adv idx: 22\n",
      "\t Adv idx: 23\n",
      "\t Adv idx: 24\n",
      "\t Adv idx: 25\n",
      "\t Adv idx: 26\n",
      "\t Adv idx: 27\n",
      "\t Adv idx: 28\n",
      "\t Adv idx: 29\n",
      "\t Adv idx: 30\n",
      "\t Adv idx: 31\n",
      "\t Adv idx: 32\n",
      "\t Adv idx: 33\n",
      "\t Adv idx: 34\n",
      "\t Adv idx: 35\n",
      "\t Adv idx: 36\n",
      "\t Adv idx: 37\n",
      "\t Adv idx: 38\n",
      "\t Adv idx: 39\n"
     ]
    }
   ],
   "source": [
    "# Run Measurements for both targetted and untargeted analysis\n",
    "new_num_models = len(new_models_test)\n",
    "victim_idxs = range(new_num_models)\n",
    "custom_batch_size = 500\n",
    "eps = 4.5\n",
    "\n",
    "\n",
    "for adv_idx in victim_idxs:\n",
    "    print(\"\\t Adv idx:\", adv_idx)\n",
    "    \n",
    "    dataloader = load_client_data(clients = clients, c_id = adv_idx, mode = 'test') # or test/train\n",
    "    \n",
    "    batch_size = min(custom_batch_size, dataloader.y_data.shape[0])\n",
    "    \n",
    "    t1 = Transferer(models_list=new_models_test, dataloader=dataloader)\n",
    "    t1.generate_victims(victim_idxs)\n",
    "    \n",
    "    # Perform Attacks\n",
    "    t1.atk_params = PGD_Params()\n",
    "    t1.atk_params.set_params(batch_size=batch_size, iteration = 10,\n",
    "                   target = 3, x_val_min = torch.min(data_x), x_val_max = torch.max(data_x),\n",
    "                   step_size = 0.01, step_norm = \"inf\", eps = eps, eps_norm = 2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    t1.generate_advNN(adv_idx)\n",
    "    t1.generate_xadv(atk_type = \"pgd\")\n",
    "    t1.send_to_victims(victim_idxs)\n",
    "\n",
    "    # Log Performance\n",
    "    logs_adv[adv_idx]['orig_acc_transfers'] = copy.deepcopy(t1.orig_acc_transfers)\n",
    "    logs_adv[adv_idx]['orig_similarities'] = copy.deepcopy(t1.orig_similarities)\n",
    "    logs_adv[adv_idx]['adv_acc_transfers'] = copy.deepcopy(t1.adv_acc_transfers)\n",
    "    logs_adv[adv_idx]['adv_similarities_target'] = copy.deepcopy(t1.adv_similarities)        \n",
    "    logs_adv[adv_idx]['adv_target'] = copy.deepcopy(t1.adv_target_hit)\n",
    "\n",
    "    # Miss attack\n",
    "    t1.atk_params.set_params(batch_size=batch_size, iteration = 10,\n",
    "                   target = -1, x_val_min = torch.min(data_x), x_val_max = torch.max(data_x),\n",
    "                   step_size = 0.01, step_norm = \"inf\", eps = eps, eps_norm = 2)\n",
    "    t1.generate_xadv(atk_type = \"pgd\")\n",
    "    t1.send_to_victims(victim_idxs)\n",
    "    logs_adv[adv_idx]['adv_miss'] = copy.deepcopy(t1.adv_acc_transfers)\n",
    "    logs_adv[adv_idx]['adv_similarities_untarget'] = copy.deepcopy(t1.adv_similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print Relevant Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['orig_acc_transfers','orig_similarities','adv_acc_transfers','adv_similarities_target',\n",
    "           'adv_similarities_untarget','adv_target','adv_miss'] #,'metric_alignment']\n",
    "\n",
    "orig_acc = np.zeros([len(victim_idxs),len(victim_idxs)]) \n",
    "orig_sim = np.zeros([len(victim_idxs),len(victim_idxs)]) \n",
    "adv_acc = np.zeros([len(victim_idxs),len(victim_idxs)]) \n",
    "adv_sim_target = np.zeros([len(victim_idxs),len(victim_idxs)]) \n",
    "adv_sim_untarget = np.zeros([len(victim_idxs),len(victim_idxs)]) \n",
    "adv_target = np.zeros([len(victim_idxs),len(victim_idxs)])\n",
    "adv_miss = np.zeros([len(victim_idxs),len(victim_idxs)]) \n",
    "\n",
    "for adv_idx in range(len(victim_idxs)):\n",
    "    for victim in range(len(victim_idxs)):\n",
    "        orig_acc[adv_idx,victim] = logs_adv[victim_idxs[adv_idx]][metrics[0]][victim_idxs[victim]].data.tolist()\n",
    "        orig_sim[adv_idx,victim] = logs_adv[victim_idxs[adv_idx]][metrics[1]][victim_idxs[victim]].data.tolist()\n",
    "        adv_acc[adv_idx,victim] = logs_adv[victim_idxs[adv_idx]][metrics[2]][victim_idxs[victim]].data.tolist()\n",
    "        adv_sim_target[adv_idx,victim] = logs_adv[victim_idxs[adv_idx]][metrics[3]][victim_idxs[victim]].data.tolist()\n",
    "        adv_sim_untarget[adv_idx,victim] = logs_adv[victim_idxs[adv_idx]][metrics[4]][victim_idxs[victim]].data.tolist()\n",
    "        adv_target[adv_idx,victim] = logs_adv[victim_idxs[adv_idx]][metrics[5]][victim_idxs[victim]].data.tolist()\n",
    "        adv_miss[adv_idx,victim] = logs_adv[victim_idxs[adv_idx]][metrics[6]][victim_idxs[victim]].data.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adv_target: 0.021070879860781135\n",
      "adv_miss: 0.672129987180233\n",
      "orig_acc: 0.7393902689218521\n"
     ]
    }
   ],
   "source": [
    "print('adv_target:', avg_nondiag(adv_target))\n",
    "print('adv_miss:', avg_nondiag(adv_miss))\n",
    "print('orig_acc:', np.mean(np.diagonal(orig_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.79 0.86 0.71 0.55 0.66 0.84 0.67 0.53 0.88 0.82 0.73 0.86 0.84 0.79\n",
      " 0.68 0.96 0.75 0.84 0.88 0.69]\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(orig_acc[:20,:20],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.79 0.78 0.62 0.46 0.56 0.74 0.53 0.42 0.88 0.74 0.67 0.80 0.73 0.69\n",
      " 0.62 0.96 0.69 0.80 0.78 0.66]\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(adv_miss[:20,:20],axis=1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ep =4\n",
    "\n",
    "# Fedavg - acc\n",
    "[0.79 0.86 0.71 0.55 0.66 0.84 0.67 0.53 0.88 0.82 0.73 0.86 0.84 0.79 0.68 0.96 0.75 0.84 0.88 0.69]\n",
    "# Fedem - acc\n",
    "[0.76 0.70 0.65 0.53 0.63 0.71 0.63 0.52 0.83 0.78 0.67 0.72 0.73 0.67 0.67 0.89 0.70 0.72 0.79 0.60]\n",
    "# local - acc\n",
    "[0.79 0.86 0.71 0.55 0.66 0.84 0.67 0.53 0.88 0.82 0.73 0.86 0.84 0.79 0.68 0.96 0.75 0.84 0.88 0.69]\n",
    "\n",
    "# Fedavg - rob (73)\n",
    "[0.79 0.84 0.62 0.46 0.56 0.74 0.53 0.44 0.88 0.79 0.67 0.80 0.72 0.69 0.62 0.96 0.69 0.80 0.78 0.66]\n",
    "# fedem - rob (71)\n",
    "[0.74 0.69 0.62 0.45 0.61 0.66 0.56 0.51 0.81 0.75 0.61 0.71 0.70 0.63 0.61 0.89 0.68 0.69 0.76 0.55]\n",
    "# local - rob (66)\n",
    "[0.79 0.78 0.62 0.46 0.56 0.74 0.53 0.42 0.88 0.74 0.67 0.80 0.73 0.69 0.62 0.96 0.69 0.80 0.78 0.66]\n",
    "\n",
    "ep = 6.5\n",
    "# Fedavg - acc\n",
    "[0.79 0.86 0.71 0.55 0.66 0.84 0.67 0.53 0.88 0.82 0.73 0.86 0.84 0.79 0.68 0.96 0.75 0.84 0.88 0.69]\n",
    "# Fedem - acc\n",
    "[0.76 0.70 0.65 0.53 0.63 0.71 0.63 0.52 0.83 0.78 0.67 0.72 0.73 0.67 0.67 0.89 0.70 0.72 0.79 0.60]\n",
    "# local - acc\n",
    "[0.79 0.78 0.62 0.44 0.56 0.74 0.55 0.44 0.88 0.76 0.67 0.77 0.72 0.68 0.62 0.96 0.69 0.78 0.75 0.66]\n",
    "\n",
    "# Fedavg - rob\n",
    "[0.79 0.81 0.62 0.46 0.56 0.75 0.53 0.44 0.88 0.76 0.67 0.77 0.72 0.65 0.64 0.96 0.69 0.78 0.78 0.66]\n",
    "# fedem - rob\n",
    "[0.74 0.68 0.63 0.44 0.61 0.64 0.56 0.51 0.81 0.74 0.62 0.71 0.69 0.64 0.62 0.88 0.67 0.70 0.75 0.54]\n",
    "# local -rob\n",
    "[0.79 0.86 0.71 0.55 0.66 0.84 0.67 0.53 0.88 0.82 0.73 0.86 0.84 0.79 0.68 0.96 0.75 0.84 0.88 0.69]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Up Interboundary Measure Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([23,  4, 25,  3,  7, 17, 28, 28, 31,  7, 30, 24, 22, 27, 35,  5, 28, 28,\n",
       "         7,  4, 30,  8,  4,  2, 14,  9, 60, 46,  7, 11,  1, 24, 38, 24, 22,  0,\n",
       "        30, 12,  6,  0,  9, 19,  7, 49, 59, 43,  3, 24, 28])"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clients[6].test_iterator.dataset.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7, 16,  7,  5,  9,  5,  2,  8, 19,  1,  6, 10,  8, 50,  8,  9, 34,  6,\n",
       "         9,  8,  7,  1,  7,  6, 17, 26,  5,  7,  3, 49, 12, 27,  3, 37, 23,  9,\n",
       "         1])"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clients[1].test_iterator.dataset.targets"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "num_trials = 50\n",
    "batch_size = 5000\n",
    "adv_idx = [0]\n",
    "\n",
    "dataloader = Custom_Dataloader(data_x, data_y)\n",
    "\n",
    "# t1 = Boundary_Transferer(models_list=models_test, dataloader=dataloader)\n",
    "# t1.base_nn_idx = 0\n",
    "# t1.victim_idx = [5]\n",
    "vidx = [1,2,3,4,5,6,7]\n",
    "\n",
    "dists_measure_legit = np.zeros([num_trials, len(vidx)])\n",
    "dists_measure_adv = np.zeros([num_trials, len(vidx)])\n",
    "dists_measure_adv_ensemble = np.zeros([num_trials, len(vidx)])\n",
    "\n",
    "for i in range(num_trials):\n",
    "    print(\"num_trial:\", i)\n",
    "    t1 = Boundary_Transferer(models_list=models_test, dataloader=dataloader)\n",
    "    t1.base_nn_idx = 0\n",
    "    t1.victim_idx = vidx\n",
    "\n",
    "    t1.atk_params = PGD_Params()\n",
    "    t1.atk_params.set_params(batch_size=500, iteration = 30,\n",
    "                   target = -1, x_val_min = torch.min(data_x), x_val_max = torch.max(data_x),\n",
    "                   step_size = 0.05, step_norm = \"inf\", eps = 3, eps_norm = 2)\n",
    "    t1.set_adv_NN(t1.base_nn_idx)\n",
    "\n",
    "    base_ep_legit, victim_eps_legit = t1.legitimate_direction(batch_size=batch_size, ep_granularity = 0.5, \n",
    "                                                              rep_padding = 1000, new_point = True,print_res = False)\n",
    "    \n",
    "    base_ep_adv, victim_eps_adv = t1.adversarial_direction(ep_granularity = 0.5, \n",
    "                                                              rep_padding = 1000, new_point = False,print_res = False)\n",
    "    \n",
    "    idx = 0\n",
    "    for key, value in victim_eps_legit.items():\n",
    "        dists_measure_legit[i,idx] = np.abs(base_ep_legit-value)\n",
    "        idx+=1\n",
    "        \n",
    "    idx = 0\n",
    "    for key, value in victim_eps_adv.items():\n",
    "        dists_measure_adv[i,idx] = np.abs(base_ep_adv - value)\n",
    "        idx+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.mean(np.average(dists_measure_legit,axis=0))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.mean(np.average(dists_measure_adv,axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Validation Dataset\n",
    "\n",
    "# Local - Benign\n",
    "local_adv_miss = 0.21# temp\n",
    "local_adv_target = 0.03#temp\n",
    "local_orig_acc = 0.61\n",
    "local_ibdist_legit = 7.615\n",
    "local_ibdist_adv = 41.505625\n",
    "\n",
    "# Fedavg - Benign\n",
    "fedavg_adv_miss = 0.34 # temp\n",
    "fedavg_adv_target = 0.10 # temp\n",
    "fedavg_orig_acc = 0.72\n",
    "fedavg_ibdist_legit = 0\n",
    "fedavg_ibdist_adv = 0 # 15.55\n",
    "\n",
    "# FedEM - Benign\n",
    "fedem_adv_miss = 0.33 # temp\n",
    "fedem_adv_target = 0.10 # temp\n",
    "fedem_orig_acc = 0.67\n",
    "fedem_ibdist_legit = 0.01125\n",
    "fedem_ibdist_adv = 13.613125\n",
    "\n",
    "# Local - adv\n",
    "local_adv_miss = \n",
    "local_adv_target = \n",
    "local_orig_acc = \n",
    "local_ibdist_legit = \n",
    "local_ibdist_adv = \n",
    "\n",
    "# Fedavg - adv\n",
    "fedavg_adv_miss = \n",
    "fedavg_adv_target =  \n",
    "fedavg_orig_acc = \n",
    "fedavg_ibdist_legit = 0\n",
    "fedavg_ibdist_adv = 0\n",
    "\n",
    "# FedEM - adv\n",
    "fedem_adv_miss = \n",
    "fedem_adv_target = \n",
    "fedem_orig_acc = \n",
    "fedem_ibdist_legit = \n",
    "fedem_ibdist_adv = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Validation Dataset\n",
    "\n",
    "# Local\n",
    "local_adv_miss = 0.37\n",
    "local_adv_target = 0.06\n",
    "local_orig_acc = 0.99\n",
    "local_ibdist_legit = \n",
    "local_ibdist_adv = \n",
    "\n",
    "# Fedavg - Benign\n",
    "fedavg_adv_miss = 0.00\n",
    "fedavg_adv_target = 0.85\n",
    "fedavg_orig_acc = 0.94\n",
    "fedavg_ibdist_legit = 0\n",
    "fedavg_ibdist_adv = 0\n",
    "\n",
    "# FedEM - Benign\n",
    "fedem_adv_miss = 0.10\n",
    "fedem_adv_target = 0.44\n",
    "fedem_orig_acc = 0.94\n",
    "fedem_ibdist_legit = \n",
    "fedem_ibdist_adv = \n",
    "\n",
    "# Local - adv\n",
    "local_adv_miss = 0.29\n",
    "local_adv_target = 0.05\n",
    "local_orig_acc = 0.77\n",
    "local_ibdist_legit = \n",
    "local_ibdist_adv = \n",
    "\n",
    "# Fedavg - adv\n",
    "fedavg_adv_miss = 0.33\n",
    "fedavg_adv_target =  0.24\n",
    "fedavg_orig_acc = 0.87\n",
    "fedavg_ibdist_legit = 0\n",
    "fedavg_ibdist_adv = 0\n",
    "\n",
    "# FedEM - adv\n",
    "fedem_adv_miss = 0.49\n",
    "fedem_adv_target = 0.07\n",
    "fedem_orig_acc = 0.84\n",
    "fedem_ibdist_legit = \n",
    "fedem_ibdist_adv = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Test (Train) Dataset\n",
    "\n",
    "# Local\n",
    "local_adv_miss = 0.38\n",
    "local_adv_target = 0.06\n",
    "local_orig_acc = 0.52\n",
    "local_ibdist_legit = \n",
    "local_ibdist_adv = \n",
    "\n",
    "# Fedavg - Benign\n",
    "fedavg_adv_miss = 0.00\n",
    "fedavg_adv_target = 0.85\n",
    "fedavg_orig_acc = 0.81\n",
    "fedavg_ibdist_legit = 0\n",
    "fedavg_ibdist_adv = 0\n",
    "\n",
    "# FedEM - Benign\n",
    "fedem_adv_miss = 0.10\n",
    "fedem_adv_target = 0.46\n",
    "fedem_orig_acc = 0.84\n",
    "fedem_ibdist_legit = \n",
    "fedem_ibdist_adv = \n",
    "\n",
    "# Local - adv\n",
    "local_adv_miss = 0.30\n",
    "local_adv_target = 0.05\n",
    "local_orig_acc = 0.46\n",
    "local_ibdist_legit = \n",
    "local_ibdist_adv = \n",
    "\n",
    "# Fedavg - adv\n",
    "fedavg_adv_miss = 0.26\n",
    "fedavg_adv_target = 0.30\n",
    "fedavg_orig_acc = 0.74\n",
    "fedavg_ibdist_legit = 0\n",
    "fedavg_ibdist_adv = 0\n",
    "\n",
    "# FedEM - adv\n",
    "fedem_adv_miss = 0.42\n",
    "fedem_adv_target = 0.09\n",
    "fedem_orig_acc = 0.74\n",
    "fedem_ibdist_legit = \n",
    "fedem_ibdist_adv = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
