{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial FedEM Client\n",
    "\n",
    "TJ Kim\n",
    "\n",
    "12.22.21\n",
    "\n",
    "### Summary:\n",
    "- Add functionality to client in FedEM System that allows for generation of adversarial examples and training on them\n",
    "- Copy original dataset to the side. The place of old dataset is replaced by the \"fractionally adversarial dataset\"\n",
    "- Random sampling of adversarial data points, remember idx so that it can be replaced in original location\n",
    "- Input --  fraction of dataset to be adversarial, slider on how much of the dataset is perturbed\n",
    "- Combining weighted hypotheses into a single adversarial neural network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/FedEM\n"
     ]
    }
   ],
   "source": [
    "cd /home/ubuntu/FedEM/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import General Libraries\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import copy\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import FedEM based Libraries\n",
    "from utils.utils import *\n",
    "from utils.constants import *\n",
    "from utils.args import *\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from run_experiment import *\n",
    "from models import *\n",
    "\n",
    "# Import Transfer Attack\n",
    "from transfer_attacks.Personalized_NN import *\n",
    "from transfer_attacks.Params import *\n",
    "from transfer_attacks.Transferer import *\n",
    "from transfer_attacks.Args import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Items for ADV Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually set argument parameters\n",
    "args_ = Args()\n",
    "args_.experiment = \"cifar10\"\n",
    "args_.method = \"FedEM\"\n",
    "args_.decentralized = False\n",
    "args_.sampling_rate = 1.0\n",
    "args_.input_dimension = None\n",
    "args_.output_dimension = None\n",
    "args_.n_learners= 3\n",
    "args_.n_rounds = 10\n",
    "args_.bz = 128\n",
    "args_.local_steps = 1\n",
    "args_.lr_lambda = 0\n",
    "args_.lr =0.03\n",
    "args_.lr_scheduler = 'multi_step'\n",
    "args_.log_freq = 10\n",
    "args_.device = 'cuda'\n",
    "args_.optimizer = 'sgd'\n",
    "args_.mu = 0\n",
    "args_.communication_probability = 0.1\n",
    "args_.q = 1\n",
    "args_.locally_tune_clients = False\n",
    "args_.seed = 1234\n",
    "args_.verbose = 1\n",
    "args_.save_path = 'weights/cifar/21_12_02_first_transfers_xadv_train_n40/'\n",
    "args_.validation = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Clients initialization..\n",
      "===> Building data iterators..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 80/80 [00:00<00:00, 236.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Initializing clients..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 80/80 [00:30<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Test Clients initialization..\n",
      "===> Building data iterators..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Initializing clients..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++\n",
      "Global..\n",
      "Train Loss: 2.292 | Train Acc: 12.159% |Test Loss: 2.292 | Test Acc: 12.248% |\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "################################################################################\n"
     ]
    }
   ],
   "source": [
    "# Generate the dummy values here\n",
    "aggregator, clients = dummy_aggregator(args_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create New Client Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "from copy import deepcopy\n",
    "from utils.torch_utils import *\n",
    "\n",
    "\n",
    "class Adv_MixtureClient(MixtureClient):\n",
    "    def __init__(\n",
    "            self,\n",
    "            learners_ensemble,\n",
    "            train_iterator,\n",
    "            val_iterator,\n",
    "            test_iterator,\n",
    "            logger,\n",
    "            local_steps,\n",
    "            tune_locally=False,\n",
    "            adv_proportion=0,\n",
    "            atk_params = None\n",
    "    ):\n",
    "        super(Adv_MixtureClient, self).__init__(\n",
    "            learners_ensemble=learners_ensemble,\n",
    "            train_iterator=train_iterator,\n",
    "            val_iterator=val_iterator,\n",
    "            test_iterator=test_iterator,\n",
    "            logger=logger,\n",
    "            local_steps=local_steps,\n",
    "            tune_locally=tune_locally\n",
    "        )\n",
    "\n",
    "        self.adv_proportion = adv_proportion\n",
    "        self.atk_params = atk_params\n",
    "        \n",
    "        # Make copy of dataset and set aside for adv training\n",
    "        self.og_dataloader = copy.deepcopy(self.train_iterator) # Update self.train_loader every iteration\n",
    "        \n",
    "        # Add adversarial client \n",
    "        combined_model = self.combine_learners_ensemble()\n",
    "        altered_dataloader = self.gen_customdataloader(self.og_dataloader)\n",
    "        self.adv_nn = Adv_NN(combined_model, altered_dataloader)\n",
    "        \n",
    "    def gen_customdataloader(self, og_dataloader):\n",
    "        # Combine Validation Data across all clients as test\n",
    "        data_x = []\n",
    "        data_y = []\n",
    "\n",
    "        for (x,y,idx) in og_dataloader.dataset:\n",
    "            data_x.append(x)\n",
    "            data_y.append(y)\n",
    "\n",
    "        data_x = torch.stack(data_x)\n",
    "        data_y = torch.stack(data_y)\n",
    "        dataloader = Custom_Dataloader(data_x, data_y)\n",
    "        \n",
    "        return dataloader\n",
    "    \n",
    "    def combine_learners_ensemble(self):\n",
    "\n",
    "        # This is where the models are stored -- one for each mixture --> learner.model for nn\n",
    "        hypotheses = self.learners_ensemble.learners\n",
    "\n",
    "        # obtain the state dict for each of the weights \n",
    "        weights_h = []\n",
    "\n",
    "        model_weights = self.learners_ensemble.learners_weights\n",
    "        \n",
    "        for h in hypotheses:\n",
    "            weights_h += [h.model.state_dict()]\n",
    "        \n",
    "        # first make the model with empty weights\n",
    "        new_model = copy.deepcopy(hypotheses[0].model)\n",
    "        new_model.eval()\n",
    "        new_weight_dict = copy.deepcopy(weights_h[0])\n",
    "        for key in weights_h[0]:\n",
    "            htemp = model_weights[0]*weights_h[0][key]\n",
    "            for i in range(1,len(model_weights)):\n",
    "                htemp+=model_weights[i]*weights_h[i][key]\n",
    "            new_weight_dict[key] = htemp\n",
    "        new_model.load_state_dict(new_weight_dict)\n",
    "        \n",
    "        return new_model\n",
    "    \n",
    "    def update_advnn(self):\n",
    "        # reassign weights after trained\n",
    "        self.adv_nn = self.combine_learnes_ensemble()\n",
    "        return\n",
    "    \n",
    "    def generate_adversarial_data(self):\n",
    "        # Generate adversarial datapoints while recognizing idx of sampled without replacement\n",
    "        \n",
    "        # Draw random idx without replacement \n",
    "        num_datapoints = self.train_iterator.dataset.targets.shape[0]\n",
    "        sample_size = int(np.ceil(num_datapoints * self.adv_proportion))\n",
    "        sample = np.random.choice(a=num_datapoints, size=sample_size)\n",
    "        x_data = self.adv_nn.dataloader.x_data[sample]\n",
    "        y_data = self.adv_nn.dataloader.y_data[sample]\n",
    "        \n",
    "        self.adv_nn.pgd_sub(self.atk_params, x_data.cuda(), y_data.cuda())\n",
    "        x_adv = self.adv_nn.x_adv\n",
    "        \n",
    "        return sample, x_adv\n",
    "    \n",
    "    def assign_advdataset(self):\n",
    "        # convert dataset to normed and replace specific datapoints\n",
    "        \n",
    "        # Flush current used dataset with original\n",
    "        self.train_iterator = copy.deepcopy(self.og_dataloader)\n",
    "        \n",
    "        # adversarial datasets loop, adjust normed and push \n",
    "        sample_id, x_adv = self.generate_adversarial_data()\n",
    "        \n",
    "        for i in range(sample_id.shape[0]):\n",
    "            idx = sample_id[i]\n",
    "            x_val_normed = x_adv[i]\n",
    "            x_val_unnorm = unnormalize_cifar10(x_val_normed)\n",
    "        \n",
    "            self.train_iterator.dataset.data[idx] = x_val_unnorm\n",
    "        \n",
    "        self.train_loader = iter(self.train_iterator)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-req of Initializing Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Building data iterators..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 80/80 [00:00<00:00, 205.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Initializing clients..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 80/80 [00:27<00:00,  2.94it/s]\n"
     ]
    }
   ],
   "source": [
    "data_dir = get_data_dir(args_.experiment)\n",
    "root_path = os.path.join(data_dir, \"train\")\n",
    "\n",
    "if \"logs_root\" in args_:\n",
    "    logs_root = args_.logs_root\n",
    "else:\n",
    "    logs_root = os.path.join(\"logs\", args_to_string(args_))\n",
    "\n",
    "print(\"===> Building data iterators..\")\n",
    "train_iterators, val_iterators, test_iterators =\\\n",
    "    get_loaders(\n",
    "        type_=LOADER_TYPE[args_.experiment],\n",
    "        root_path=root_path,\n",
    "        batch_size=args_.bz,\n",
    "        is_validation=args_.validation\n",
    "    )\n",
    "\n",
    "print(\"===> Initializing clients..\")\n",
    "clients_ = []\n",
    "for task_id, (train_iterator, val_iterator, test_iterator) in \\\n",
    "        enumerate(tqdm(zip(train_iterators, val_iterators, test_iterators), total=len(train_iterators))):\n",
    "\n",
    "    if train_iterator is None or test_iterator is None:\n",
    "        continue\n",
    "\n",
    "    learners_ensemble =\\\n",
    "        get_learners_ensemble(\n",
    "            n_learners=args_.n_learners,\n",
    "            name=args_.experiment,\n",
    "            device=args_.device,\n",
    "            optimizer_name=args_.optimizer,\n",
    "            scheduler_name=args_.lr_scheduler,\n",
    "            initial_lr=args_.lr,\n",
    "            input_dim=args_.input_dimension,\n",
    "            output_dim=args_.output_dimension,\n",
    "            n_rounds=args_.n_rounds,\n",
    "            seed=args_.seed,\n",
    "            mu=args_.mu\n",
    "        )\n",
    "\n",
    "    logs_path = os.path.join(logs_root, \"task_{}\".format(task_id))\n",
    "    os.makedirs(logs_path, exist_ok=True)\n",
    "    logger = SummaryWriter(logs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "atk_params = PGD_Params()\n",
    "\n",
    "atk_params.set_params(batch_size=500, iteration = 30,\n",
    "                   target = -1, x_val_min = -2, x_val_max = 2,\n",
    "                   step_size = 0.05, step_norm = \"inf\", eps = 4.5, eps_norm = \"inf\")\n",
    "\n",
    "client1 = Adv_MixtureClient(\n",
    "            learners_ensemble=learners_ensemble,\n",
    "            train_iterator=train_iterator,\n",
    "            val_iterator=val_iterator,\n",
    "            test_iterator=test_iterator,\n",
    "            logger=logger,\n",
    "            local_steps=args_.local_steps,\n",
    "            tune_locally=args_.locally_tune_clients,\n",
    "            adv_proportion = 0.3,\n",
    "            atk_params=atk_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "client1.assign_advdataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FedEM_env",
   "language": "python",
   "name": "fedem_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
