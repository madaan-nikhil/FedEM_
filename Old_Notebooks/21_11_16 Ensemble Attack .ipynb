{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Attack Implementation\n",
    "\n",
    "TJ Kim, 11.16.21\n",
    "\n",
    "#### Summary:\n",
    "- Build upon the \"Transferer\" class that performs adversarial perturbations based on gradients of one model and sends adversarial examples to other members of ensemble\n",
    "- Ensemble Cascade Adversarial Attack using multiple members of ensembles as adversarial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/FedEM\n"
     ]
    }
   ],
   "source": [
    "cd /home/ubuntu/FedEM/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import General Libraries\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import copy\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import FedEM based Libraries\n",
    "from utils.utils import *\n",
    "from utils.constants import *\n",
    "from utils.args import *\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from run_experiment import *\n",
    "from models import *\n",
    "\n",
    "# Import Transfer Attack\n",
    "from transfer_attacks.Personalized_NN import *\n",
    "from transfer_attacks.Params import *\n",
    "from transfer_attacks.Transferer import *\n",
    "from transfer_attacks.Args import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build New Ensemble Transferer Subclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ensemble_Transferer(Transferer): \n",
    "    def __init__(self, models_list, dataloader):\n",
    "        super(Ensemble_Transferer, self).__init__(models_list, dataloader)\n",
    "    \n",
    "        # Other Params\n",
    "        self.advNN_idx = None # list\n",
    "        self.advNN = None # dict of personalized_NN\n",
    "        self.atk_order = None\n",
    "        \n",
    "    def generate_advNN(self, client_idx):\n",
    "        \"\"\"\n",
    "        Select specific client to load neural network to \n",
    "        Load the data for that client\n",
    "        Lod the weights for that client\n",
    "        This is the client that will generate perturbations\n",
    "        \"\"\"        \n",
    "        # Import the loader for this dataset only\n",
    "        \n",
    "        self.advNN_idx = client_idx # List\n",
    "        self.advNN = {} # Dict of NN\n",
    "        \n",
    "        for i in client_idx:\n",
    "            self.advNN[i] = copy.deepcopy(Adv_NN(self.models_list[i], self.dataloader))\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def choose_attack_sequence(self):\n",
    "        \"\"\"\n",
    "        Given the number of iterations of attack in params, and number of adv nn\n",
    "        choose a list of len~iter of which idx to attack\n",
    "        must run generate_advNN prior \n",
    "        \"\"\"\n",
    "    \n",
    "        num_iters = self.atk_params.iteration\n",
    "        atk_order = []\n",
    "        \n",
    "        for t in range(num_iters):\n",
    "            idx = t%len(self.advNN_idx)\n",
    "            atk_order += [self.advNN_idx[idx]]\n",
    "            \n",
    "        self.atk_order = atk_order\n",
    "        \n",
    "    \n",
    "    def generate_xadv(self, atk_type = \"IFSGM\", mode='test'):\n",
    "        \"\"\"\n",
    "        Generate perturbed images\n",
    "        atk_type - \"IFSGM\"\n",
    "        \"\"\"\n",
    "        \n",
    "        # Import attack parameters\n",
    "        batch_size = self.atk_params.batch_size\n",
    "        \n",
    "        # Load data to perturb\n",
    "        data_x, data_y = self.dataloader.load_batch(batch_size, mode=mode)\n",
    "        self.x_orig  = data_x.reshape(batch_size,3,32,32)\n",
    "        self.y_true = data_y.type(torch.LongTensor)\n",
    "        self.y_orig = self.y_true # Unused dummy feature\n",
    "        self.y_adv = self.y_true\n",
    "        \n",
    "        x_in = copy.deepcopy(self.x_orig)\n",
    "        y_in  = self.y_true\n",
    "        \n",
    "        # Alter number of iteration in params to 1 \n",
    "        temp_params = copy.deepcopy(self.atk_params)\n",
    "        temp_params.iteration = 1\n",
    "        \n",
    "        if atk_type == \"PGD\":\n",
    "            for idx in self.atk_order:\n",
    "                self.advNN[idx].pgd_sub(atk_params=temp_params, x_in=x_in, y_in=y_in, x_base = self.x_orig)\n",
    "                x_in = copy.deepcopy(self.advNN[idx].x_adv)\n",
    "        else:\n",
    "            for idx in self.atk_order:\n",
    "                self.advNN[idx].i_fgsm_sub(temp_params, x_in, y_in)\n",
    "                x_in = copy.deepcopy(self.advNN[idx].x_adv)\n",
    "        \n",
    "        # Record relevant tensors\n",
    "        self.x_adv = copy.deepcopy(x_in).cuda()\n",
    "        \n",
    "    def send_to_victims(self, client_idxs):\n",
    "        \"\"\"\n",
    "        Send pre-generated adversarial perturbations \n",
    "        client_idxs - list of indices of clients we want to attack (just victims)\n",
    "        \n",
    "        Then record the attack success stats accordingly\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in client_idxs:\n",
    "            self.victims[i].ensemble_forward_transfer(self.x_orig, self.x_adv, self.y_orig, self.atk_params.target)\n",
    "\n",
    "            # Record Performance\n",
    "            self.orig_target_hit[i] = self.victims[i].orig_target_achieve\n",
    "            self.adv_target_hit[i] = self.victims[i].adv_target_achieve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FedEM Analysis \n",
    "- Generate aggregator and extract saved models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Clients initialization..\n",
      "===> Building data iterators..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 80/80 [00:00<00:00, 230.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Initializing clients..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 80/80 [00:32<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Test Clients initialization..\n",
      "===> Building data iterators..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Initializing clients..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++\n",
      "Global..\n",
      "Train Loss: 2.292 | Train Acc: 12.159% |Test Loss: 2.292 | Test Acc: 12.248% |\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "################################################################################\n"
     ]
    }
   ],
   "source": [
    "# Manually set argument parameters\n",
    "args_ = Args()\n",
    "args_.experiment = \"cifar10\"\n",
    "args_.method = \"FedEM\"\n",
    "args_.decentralized = False\n",
    "args_.sampling_rate = 1.0\n",
    "args_.input_dimension = None\n",
    "args_.output_dimension = None\n",
    "args_.n_learners= 3\n",
    "args_.n_rounds = 10\n",
    "args_.bz = 128\n",
    "args_.local_steps = 1\n",
    "args_.lr_lambda = 0\n",
    "args_.lr =0.03\n",
    "args_.lr_scheduler = 'multi_step'\n",
    "args_.log_freq = 10\n",
    "args_.device = 'cuda'\n",
    "args_.optimizer = 'sgd'\n",
    "args_.mu = 0\n",
    "args_.communication_probability = 0.1\n",
    "args_.q = 1\n",
    "args_.locally_tune_clients = False\n",
    "args_.seed = 1234\n",
    "args_.verbose = 1\n",
    "args_.save_path = 'weights/cifar/21_09_28_first_transfers/'\n",
    "args_.validation = False\n",
    "\n",
    "# Generate the dummy values here\n",
    "aggregator, clients = dummy_aggregator(args_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT DATASET -- DON'T IMPORT FOR FED LOCAL CASE LATER\n",
    "\n",
    "# Combine Validation Data across all clients as test\n",
    "data_x = []\n",
    "data_y = []\n",
    "\n",
    "for i in range(len(clients)):\n",
    "    daniloader = clients[i].val_iterator\n",
    "    for (x,y,idx) in daniloader.dataset:\n",
    "        data_x.append(x)\n",
    "        data_y.append(y)\n",
    "\n",
    "data_x = torch.stack(data_x)\n",
    "data_y = torch.stack(data_y)\n",
    "\n",
    "# Create dataloader from validation dataset that allows for diverse batch size\n",
    "dataloader = Custom_Dataloader(data_x, data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import weights for aggregator\n",
    "aggregator.load_state(args_.save_path)\n",
    "\n",
    "# This is where the models are stored -- one for each mixture --> learner.model for nn\n",
    "hypotheses = aggregator.global_learners_ensemble.learners\n",
    "\n",
    "# obtain the state dict for each of the weights \n",
    "weights_h = []\n",
    "\n",
    "for h in hypotheses:\n",
    "    weights_h += [h.model.state_dict()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.load(\"weights/cifar/21_09_28_first_transfers/train_client_weights.npy\")\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.2f}\".format(x)})\n",
    "\n",
    "#print(weights)\n",
    "\n",
    "# Set model weights\n",
    "model_weights = []\n",
    "num_models = 7\n",
    "\n",
    "for i in range(num_models):\n",
    "    model_weights += [weights[i]]\n",
    "    \n",
    "    \n",
    "# Generate the weights to test on as linear combinations of the model_weights\n",
    "models_test = []\n",
    "\n",
    "for (w0,w1,w2) in model_weights:\n",
    "    # first make the model with empty weights\n",
    "    new_model = copy.deepcopy(hypotheses[0].model)\n",
    "    new_model.eval()\n",
    "    new_weight_dict = copy.deepcopy(weights_h[0])\n",
    "    for key in weights_h[0]:\n",
    "        new_weight_dict[key] = w0*weights_h[0][key] + w1*weights_h[1][key] + w2*weights_h[2][key]\n",
    "    new_model.load_state_dict(new_weight_dict)\n",
    "    models_test += [new_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Transfer Attack Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Up Dictionaries -- list holds the adversary idx\n",
    "logs_adv = []\n",
    "\n",
    "for i in range(len(model_weights)):\n",
    "    adv_dict = {}\n",
    "    adv_dict['orig_target_hit'] = None\n",
    "    adv_dict['adv_target_hit'] = None\n",
    "    logs_adv += [adv_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_idx = [0,1,2]\n",
    "victim_idxs = [0,1,2,3,4,5,6]\n",
    "\n",
    "t1 = Ensemble_Transferer(models_list=models_test, dataloader=dataloader)\n",
    "\n",
    "# atk Params setup\n",
    "t1.atk_params = PGD_Params()\n",
    "t1.atk_params.set_params(batch_size=500, iteration = 30,\n",
    "                   target = 9, x_val_min = torch.min(data_x), x_val_max = torch.max(data_x),\n",
    "                   step_size = 0.01, step_norm = \"inf\", eps = 0.5, eps_norm = \"inf\")\n",
    "\n",
    "\n",
    "# t1.atk_params = IFSGM_Params()\n",
    "# t1.atk_params.set_params(batch_size=500, eps=0.1, alpha=0.05, iteration = 30,\n",
    "#                target = 9, x_val_min = torch.min(data_x), x_val_max = torch.max(data_x))\n",
    "\n",
    "t1.generate_victims(victim_idxs)\n",
    "t1.generate_advNN(adv_idx)\n",
    "t1.advNN_idx = adv_idx\n",
    "t1.choose_attack_sequence()\n",
    "t1.generate_xadv(atk_type = \"PGD\")\n",
    "t1.send_to_victims(victim_idxs)\n",
    "\n",
    "# Log Performance\n",
    "# logs_adv[adv_idx]['orig_target_hit'] = t1.orig_target_hit\n",
    "# logs_adv[adv_idx]['adv_target_hit'] = t1.adv_target_hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: tensor(0.1080, device='cuda:0'),\n",
       " 1: tensor(0.1260, device='cuda:0'),\n",
       " 2: tensor(0.0980, device='cuda:0'),\n",
       " 3: tensor(0.0980, device='cuda:0'),\n",
       " 4: tensor(0.0900, device='cuda:0'),\n",
       " 5: tensor(0.1200, device='cuda:0'),\n",
       " 6: tensor(0.1120, device='cuda:0')}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.orig_target_hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: tensor(0., device='cuda:0'),\n",
       " 1: tensor(0., device='cuda:0'),\n",
       " 2: tensor(0., device='cuda:0'),\n",
       " 3: tensor(0., device='cuda:0'),\n",
       " 4: tensor(0., device='cuda:0'),\n",
       " 5: tensor(0., device='cuda:0'),\n",
       " 6: tensor(0., device='cuda:0')}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.adv_target_hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 5, 3, 7, 7, 5, 6, 4, 8, 7, 4, 2, 2, 3, 2, 8, 4, 9, 1, 5, 4, 0, 3, 7,\n",
       "        2, 8, 9, 0, 2, 9, 0, 0, 8, 8, 1, 4, 0, 9, 8, 6, 5, 4, 5, 6, 8, 3, 0, 2,\n",
       "        0, 7, 4, 6, 8, 0, 9, 6, 0, 8, 2, 3, 2, 5, 0, 5, 8, 9, 9, 2, 0, 0, 4, 0,\n",
       "        7, 4, 1, 0, 5, 1, 7, 6, 9, 0, 3, 5, 3, 8, 8, 3, 6, 4, 6, 9, 8, 6, 7, 8,\n",
       "        4, 7, 3, 1, 7, 5, 7, 3, 2, 7, 5, 7, 9, 8, 6, 2, 6, 7, 4, 9, 3, 1, 8, 5,\n",
       "        9, 3, 9, 4, 5, 7, 0, 7, 6, 1, 3, 2, 1, 3, 6, 8, 9, 7, 3, 7, 8, 0, 5, 4,\n",
       "        4, 8, 6, 3, 1, 0, 1, 6, 1, 6, 1, 4, 5, 1, 0, 8, 6, 5, 8, 9, 8, 1, 0, 2,\n",
       "        7, 2, 7, 1, 8, 8, 0, 8, 8, 0, 4, 3, 1, 3, 6, 4, 3, 5, 0, 5, 4, 9, 8, 5,\n",
       "        7, 6, 7, 9, 7, 0, 6, 8, 5, 5, 5, 4, 9, 9, 7, 4, 2, 4, 9, 7, 1, 1, 0, 3,\n",
       "        2, 1, 5, 5, 0, 1, 9, 4, 2, 3, 7, 8, 1, 8, 0, 0, 7, 0, 6, 9, 8, 3, 4, 8,\n",
       "        6, 8, 9, 0, 8, 5, 9, 6, 6, 4, 4, 5, 9, 8, 1, 9, 8, 9, 4, 5, 6, 5, 8, 9,\n",
       "        4, 2, 8, 7, 2, 5, 6, 2, 7, 5, 9, 5, 4, 1, 8, 1, 4, 8, 6, 4, 4, 6, 0, 8,\n",
       "        4, 2, 2, 8, 4, 8, 4, 7, 5, 7, 0, 9, 1, 2, 3, 8, 7, 8, 3, 1, 8, 2, 2, 6,\n",
       "        9, 5, 1, 0, 8, 8, 0, 0, 5, 3, 0, 5, 7, 8, 7, 4, 7, 2, 1, 9, 6, 6, 2, 0,\n",
       "        8, 2, 0, 9, 8, 5, 6, 5, 3, 0, 6, 8, 9, 1, 0, 8, 5, 5, 6, 6, 1, 1, 9, 9,\n",
       "        2, 0, 6, 7, 9, 7, 0, 5, 4, 5, 0, 4, 4, 1, 6, 1, 0, 0, 4, 0, 9, 6, 0, 3,\n",
       "        2, 5, 1, 0, 8, 2, 0, 9, 0, 6, 3, 0, 5, 1, 4, 7, 7, 0, 5, 1, 4, 9, 3, 8,\n",
       "        6, 9, 6, 6, 3, 8, 2, 5, 2, 0, 2, 4, 5, 5, 6, 2, 5, 6, 5, 8, 0, 6, 7, 7,\n",
       "        8, 3, 3, 4, 9, 4, 1, 5, 7, 1, 2, 8, 4, 4, 5, 0, 4, 7, 9, 0, 4, 8, 5, 7,\n",
       "        0, 7, 3, 9, 6, 8, 7, 9, 3, 6, 1, 0, 3, 5, 5, 4, 5, 5, 9, 2, 1, 6, 5, 0,\n",
       "        8, 4, 9, 3, 2, 7, 8, 5, 9, 4, 7, 8, 1, 2, 4, 3, 7, 3, 7, 4])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.y_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 5, 3, 7, 7, 5, 6, 4, 8, 7, 4, 2, 2, 3, 2, 8, 4, 9, 1, 5, 4, 0, 3, 7,\n",
       "        2, 8, 9, 0, 2, 9, 0, 0, 8, 8, 1, 4, 0, 9, 8, 6, 5, 4, 5, 6, 8, 3, 0, 2,\n",
       "        0, 7, 4, 6, 8, 0, 9, 6, 0, 8, 2, 3, 2, 5, 0, 5, 8, 9, 9, 2, 0, 0, 4, 0,\n",
       "        7, 4, 1, 0, 5, 1, 7, 6, 9, 0, 3, 5, 3, 8, 8, 3, 6, 4, 6, 9, 8, 6, 7, 8,\n",
       "        4, 7, 3, 1, 7, 5, 7, 3, 2, 7, 5, 7, 9, 8, 6, 2, 6, 7, 4, 9, 3, 1, 8, 5,\n",
       "        9, 3, 9, 4, 5, 7, 0, 7, 6, 1, 3, 2, 1, 3, 6, 8, 9, 7, 3, 7, 8, 0, 5, 4,\n",
       "        4, 8, 6, 3, 1, 0, 1, 6, 1, 6, 1, 4, 5, 1, 0, 8, 6, 5, 8, 9, 8, 1, 0, 2,\n",
       "        7, 2, 7, 1, 8, 8, 0, 8, 8, 0, 4, 3, 1, 3, 6, 4, 3, 5, 0, 5, 4, 9, 8, 5,\n",
       "        7, 6, 7, 9, 7, 0, 6, 8, 5, 5, 5, 4, 9, 9, 7, 4, 2, 4, 9, 7, 1, 1, 0, 3,\n",
       "        2, 1, 5, 5, 0, 1, 9, 4, 2, 3, 7, 8, 1, 8, 0, 0, 7, 0, 6, 9, 8, 3, 4, 8,\n",
       "        6, 8, 9, 0, 8, 5, 9, 6, 6, 4, 4, 5, 9, 8, 1, 9, 8, 9, 4, 5, 6, 5, 8, 9,\n",
       "        4, 2, 8, 7, 2, 5, 6, 2, 7, 5, 9, 5, 4, 1, 8, 1, 4, 8, 6, 4, 4, 6, 0, 8,\n",
       "        4, 2, 2, 8, 4, 8, 4, 7, 5, 7, 0, 9, 1, 2, 3, 8, 7, 8, 3, 1, 8, 2, 2, 6,\n",
       "        9, 5, 1, 0, 8, 8, 0, 0, 5, 3, 0, 5, 7, 8, 7, 4, 7, 2, 1, 9, 6, 6, 2, 0,\n",
       "        8, 2, 0, 9, 8, 5, 6, 5, 3, 0, 6, 8, 9, 1, 0, 8, 5, 5, 6, 6, 1, 1, 9, 9,\n",
       "        2, 0, 6, 7, 9, 7, 0, 5, 4, 5, 0, 4, 4, 1, 6, 1, 0, 0, 4, 0, 9, 6, 0, 3,\n",
       "        2, 5, 1, 0, 8, 2, 0, 9, 0, 6, 3, 0, 5, 1, 4, 7, 7, 0, 5, 1, 4, 9, 3, 8,\n",
       "        6, 9, 6, 6, 3, 8, 2, 5, 2, 0, 2, 4, 5, 5, 6, 2, 5, 6, 5, 8, 0, 6, 7, 7,\n",
       "        8, 3, 3, 4, 9, 4, 1, 5, 7, 1, 2, 8, 4, 4, 5, 0, 4, 7, 9, 0, 4, 8, 5, 7,\n",
       "        0, 7, 3, 9, 6, 8, 7, 9, 3, 6, 1, 0, 3, 5, 5, 4, 5, 5, 9, 2, 1, 6, 5, 0,\n",
       "        8, 4, 9, 3, 2, 7, 8, 5, 9, 4, 7, 8, 1, 2, 4, 3, 7, 3, 7, 4])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.y_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FedLocal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Clients initialization..\n",
      "===> Building data iterators..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 80/80 [00:00<00:00, 243.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Initializing clients..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 80/80 [00:09<00:00,  8.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Test Clients initialization..\n",
      "===> Building data iterators..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Initializing clients..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++\n",
      "Global..\n",
      "Train Loss: 2.299 | Train Acc: 10.643% |Test Loss: 2.298 | Test Acc: 10.503% |\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "################################################################################\n"
     ]
    }
   ],
   "source": [
    "# Manually set argument parameters\n",
    "args_ = Args()\n",
    "args_.experiment = \"cifar10\"\n",
    "args_.method = \"local\"\n",
    "args_.decentralized = False\n",
    "args_.sampling_rate = 1.0\n",
    "args_.input_dimension = None\n",
    "args_.output_dimension = None\n",
    "args_.n_learners= 1\n",
    "args_.n_rounds = 10\n",
    "args_.bz = 128\n",
    "args_.local_steps = 1\n",
    "args_.lr_lambda = 0\n",
    "args_.lr =0.03\n",
    "args_.lr_scheduler = 'multi_step'\n",
    "args_.log_freq = 10\n",
    "args_.device = 'cuda'\n",
    "args_.optimizer = 'sgd'\n",
    "args_.mu = 0\n",
    "args_.communication_probability = 0.1\n",
    "args_.q = 1\n",
    "args_.locally_tune_clients = False\n",
    "args_.seed = 1234\n",
    "args_.verbose = 1\n",
    "args_.save_path = 'weights/cifar/21_11_06_local/'\n",
    "args_.validation = False\n",
    "\n",
    "# Generate the dummy values here\n",
    "aggregator, clients = dummy_aggregator(args_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import weights for aggregator\n",
    "aggregator.load_state(args_.save_path)\n",
    "\n",
    "# Set model weights\n",
    "weights = np.load(\"weights/cifar/21_11_06_local/train_client_weights.npy\")\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.2f}\".format(x)})\n",
    "\n",
    "model_weights = []\n",
    "num_models = 7\n",
    "\n",
    "for i in range(num_models):\n",
    "    model_weights += [weights[i]]\n",
    "\n",
    "# Generate the weights to test on as linear combinations of the model_weights\n",
    "models_test = []\n",
    "\n",
    "for i in range(num_models):\n",
    "    new_model = copy.deepcopy(aggregator.clients[i].learners_ensemble.learners[0].model)\n",
    "    new_model.eval()\n",
    "    models_test += [new_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Up Dictionaries -- list holds the adversary idx\n",
    "logs_adv = []\n",
    "\n",
    "for i in range(len(model_weights)):\n",
    "    adv_dict = {}\n",
    "    adv_dict['orig_target_hit'] = None\n",
    "    adv_dict['adv_target_hit'] = None\n",
    "    logs_adv += [adv_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_idx = [0,1,2]\n",
    "victim_idxs = [0,1,2,3,4,5,6]\n",
    "\n",
    "t1 = Ensemble_Transferer(models_list=models_test, dataloader=dataloader)\n",
    "\n",
    "# atk Params setup\n",
    "t1.atk_params = PGD_Params()\n",
    "t1.atk_params.set_params(batch_size=500, iteration = 30,\n",
    "                   target = 9, x_val_min = torch.min(data_x), x_val_max = torch.max(data_x),\n",
    "                   step_size = 0.01, step_norm = \"inf\", eps = 0.3, eps_norm = 2)\n",
    "\n",
    "t1.generate_victims(victim_idxs)\n",
    "t1.generate_advNN(adv_idx)\n",
    "t1.advNN_idx = adv_idx\n",
    "t1.choose_attack_sequence()\n",
    "t1.generate_xadv(atk_type = \"PGD\")\n",
    "t1.send_to_victims(victim_idxs)\n",
    "\n",
    "# Log Performance\n",
    "# logs_adv[adv_idx]['orig_target_hit'] = t1.orig_target_hit\n",
    "# logs_adv[adv_idx]['adv_target_hit'] = t1.adv_target_hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: tensor(0.1540, device='cuda:0'),\n",
       " 1: tensor(0.2300, device='cuda:0'),\n",
       " 2: tensor(0.1400, device='cuda:0'),\n",
       " 3: tensor(0.1460, device='cuda:0'),\n",
       " 4: tensor(0.0260, device='cuda:0'),\n",
       " 5: tensor(0.1340, device='cuda:0'),\n",
       " 6: tensor(0.1540, device='cuda:0')}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.orig_target_hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: tensor(0., device='cuda:0'),\n",
       " 1: tensor(0.0020, device='cuda:0'),\n",
       " 2: tensor(0., device='cuda:0'),\n",
       " 3: tensor(0.0020, device='cuda:0'),\n",
       " 4: tensor(0., device='cuda:0'),\n",
       " 5: tensor(0., device='cuda:0'),\n",
       " 6: tensor(0., device='cuda:0')}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.adv_target_hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4806)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(t1.dataloader.x_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1223, -0.1029, -0.1416,  ..., -1.3823, -1.7700, -1.5955],\n",
       "         [-0.0447, -0.0447, -0.0641,  ..., -1.4211, -1.8281, -1.5761],\n",
       "         [ 0.0522,  0.0522,  0.0134,  ..., -1.3823, -1.9057, -1.5761],\n",
       "         ...,\n",
       "         [-0.1416, -0.5875, -1.2078,  ..., -1.8281, -2.2352, -2.1577],\n",
       "         [-0.8783, -0.2967, -0.0447,  ..., -1.7894, -2.0414, -2.0220],\n",
       "         [-1.3241, -0.6844, -0.3161,  ..., -1.6924, -1.9444, -2.0026]],\n",
       "\n",
       "        [[-0.1959, -0.1959, -0.2352,  ..., -1.7496, -1.9659, -1.8873],\n",
       "         [-0.1566, -0.1369, -0.1566,  ..., -1.8282, -2.0446, -1.9069],\n",
       "         [-0.0386, -0.0582, -0.0779,  ..., -1.8676, -2.1429, -1.8873],\n",
       "         ...,\n",
       "         [-0.4712, -0.9039, -1.7299,  ..., -2.1233, -2.3789, -2.3396],\n",
       "         [-1.6316, -1.2382, -0.8842,  ..., -2.1233, -2.2216, -2.2216],\n",
       "         [-1.7496, -1.3562, -0.6089,  ..., -2.0249, -2.1429, -2.2019]],\n",
       "\n",
       "        [[-0.2899, -0.2899, -0.3094,  ..., -1.9092, -1.9482, -1.9873],\n",
       "         [-0.2509, -0.2313, -0.2313,  ..., -2.0458, -2.0458, -2.0263],\n",
       "         [-0.1728, -0.1728, -0.1923,  ..., -2.0848, -2.1238, -2.0653],\n",
       "         ...,\n",
       "         [-0.6215, -1.1288, -1.8702,  ..., -2.0653, -2.2019, -2.1824],\n",
       "         [-1.9678, -1.7141, -1.4020,  ..., -2.1043, -2.1238, -2.0848],\n",
       "         [-1.5385, -1.3044, -0.6801,  ..., -2.0263, -2.0653, -2.0653]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.x_orig[3] # - t1.x_orig[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.4291, -2.4291, -2.4291,  ..., -2.4291, -2.4291, -2.4291],\n",
       "         [-2.4291, -2.4291, -2.4291,  ..., -2.4291, -2.4291, -2.4291],\n",
       "         [-2.4291, -2.4291, -2.4291,  ..., -2.4291, -2.4291, -2.4291],\n",
       "         ...,\n",
       "         [-2.4291, -2.4291, -2.4291,  ..., -2.4291, -2.4291, -2.4291],\n",
       "         [-2.4291, -2.4291, -2.4291,  ..., -2.4291, -2.4291, -2.4291],\n",
       "         [-2.4291, -2.4291, -2.4291,  ..., -2.4291, -2.4291, -2.4291]],\n",
       "\n",
       "        [[-2.4291, -2.4291, -2.4291,  ..., -2.4291, -2.4291, -2.4291],\n",
       "         [-2.4291, -2.4291, -2.4291,  ..., -2.4291, -2.4291, -2.4291],\n",
       "         [-2.4291, -2.4291, -2.4291,  ..., -2.4291, -2.4291, -2.4291],\n",
       "         ...,\n",
       "         [-2.4291, -2.4291, -2.4291,  ..., -2.4291, -2.4291, -2.4291],\n",
       "         [-2.4291, -2.4291, -2.4291,  ..., -2.4291, -2.4291, -2.4291],\n",
       "         [-2.4291, -2.4291, -2.4291,  ..., -2.4291, -2.4291, -2.4291]],\n",
       "\n",
       "        [[-2.4291, -2.4291, -2.4291,  ..., -2.4291, -2.4291, -2.4291],\n",
       "         [-2.4291, -2.4291, -2.4291,  ..., -2.4291, -2.4291, -2.4291],\n",
       "         [-2.4291, -2.4291, -2.4291,  ..., -2.4291, -2.4291, -2.4291],\n",
       "         ...,\n",
       "         [-2.4291, -2.4291, -2.4291,  ..., -2.4291, -2.4291, -2.4291],\n",
       "         [-2.4291, -2.4291, -2.4291,  ..., -2.4291, -2.4291, -2.4291],\n",
       "         [-2.4291, -2.4291, -2.4291,  ..., -2.4291, -2.4291, -2.4291]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.advNN[0].x_adv[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FedEM_env",
   "language": "python",
   "name": "fedem_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
