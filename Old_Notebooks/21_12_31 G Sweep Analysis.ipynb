{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# G Sweep Analysis\n",
    "TJ Kim\n",
    "\n",
    "12.31.21\n",
    "\n",
    "#### Summary:\n",
    "- Run tests on 5 neural networks of different adversarial values G = [0, 0.25, 0.5, 0.75, 1]\n",
    "- Resource has been set at E[Ru] = 0.5 in [0.2, 0.8]\n",
    "- Relevant metrics are all global (accuracy, target hit, target miss, adversarial/benign distance, gradient alignment, sim benign/adv)\n",
    "- Make new function - avg_nodiag(nparray/panda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/FedEM\n"
     ]
    }
   ],
   "source": [
    "cd /home/ubuntu/FedEM/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import General Libraries\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import copy\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import FedEM based Libraries\n",
    "from utils.utils import *\n",
    "from utils.constants import *\n",
    "from utils.args import *\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from run_experiment import *\n",
    "from models import *\n",
    "\n",
    "# Import Transfer Attack\n",
    "from transfer_attacks.Personalized_NN import *\n",
    "from transfer_attacks.Params import *\n",
    "from transfer_attacks.Transferer import *\n",
    "from transfer_attacks.Args import *\n",
    "\n",
    "from transfer_attacks.TA_utils import *\n",
    "from transfer_attacks.Boundary_Transferer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function - Calculate Mean without Diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform np.mean without the diagonal\n",
    "def avg_nondiag(array2d):\n",
    "    d1 = array2d.shape[0]\n",
    "    d2 = array2d.shape[1]\n",
    "    \n",
    "    counter = 0\n",
    "    val = 0\n",
    "    \n",
    "    for i1 in range(d1):\n",
    "        for i2 in range(d2):\n",
    "            if i1 != i2:\n",
    "                counter+=1\n",
    "                val += array2d[i1,i2]\n",
    "    \n",
    "    return val/counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Dummy Aggregator and Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Clients initialization..\n",
      "===> Building data iterators..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 80/80 [00:00<00:00, 176.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Initializing clients..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 80/80 [00:52<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Test Clients initialization..\n",
      "===> Building data iterators..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Initializing clients..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++\n",
      "Global..\n",
      "Train Loss: 2.292 | Train Acc: 12.195% |Test Loss: 2.292 | Test Acc: 12.291% |\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "################################################################################\n"
     ]
    }
   ],
   "source": [
    "# Manually set argument parameters\n",
    "args_ = Args()\n",
    "args_.experiment = \"cifar10\"\n",
    "args_.method = \"FedEM_adv\"\n",
    "args_.decentralized = False\n",
    "args_.sampling_rate = 1.0\n",
    "args_.input_dimension = None\n",
    "args_.output_dimension = None\n",
    "args_.n_learners= 3\n",
    "args_.n_rounds = 10\n",
    "args_.bz = 128\n",
    "args_.local_steps = 1\n",
    "args_.lr_lambda = 0\n",
    "args_.lr =0.03\n",
    "args_.lr_scheduler = 'multi_step'\n",
    "args_.log_freq = 10\n",
    "args_.device = 'cuda'\n",
    "args_.optimizer = 'sgd'\n",
    "args_.mu = 0\n",
    "args_.communication_probability = 0.1\n",
    "args_.q = 1\n",
    "args_.locally_tune_clients = False\n",
    "args_.seed = 1234\n",
    "args_.verbose = 1\n",
    "args_.save_path = 'weights/cifar/21_12_30_feddef2_n40_linf0_5/'\n",
    "args_.validation = False\n",
    "args_.num_user = 40\n",
    "\n",
    "# Generate the dummy values here\n",
    "aggregator, clients = dummy_aggregator(args_, num_user=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dataset From Client Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Validation Data across all clients as test\n",
    "data_x = []\n",
    "data_y = []\n",
    "\n",
    "for i in range(len(clients)):\n",
    "    daniloader = clients[i].val_iterator\n",
    "    for (x,y,idx) in daniloader.dataset:\n",
    "        data_x.append(x)\n",
    "        data_y.append(y)\n",
    "\n",
    "data_x = torch.stack(data_x)\n",
    "data_y = torch.stack(data_y)\n",
    "\n",
    "# Create dataloader from validation dataset that allows for diverse batch size\n",
    "dataloader = Custom_Dataloader(data_x, data_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Experiment Name Information\n",
    "\n",
    "Used later to loop through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_names = ['21_12_30_feddef_n40_linf0_5_G0_0/','21_12_30_feddef_n40_linf0_5_G0_25/', '21_12_30_feddef_n40_linf0_5/',\n",
    "#              '21_12_30_feddef_n40_linf0_5_G0_75/', '21_12_30_feddef_n40_linf0_5_G1_0/']\n",
    "\n",
    "# R - Sweep Resources\n",
    "# exp_names = ['22_01_01_feddef_n40_linf0_5_G0_5_R0_1/','22_01_01_feddef_n40_linf0_5_G0_5_R0_2/',\n",
    "#              '22_01_01_feddef_n40_linf0_5_G0_5_R0_4/', '21_12_30_feddef_n40_linf0_5/', \n",
    "#              '22_01_01_feddef_n40_linf0_5_G0_5_R0_6/']\n",
    "\n",
    "# G - Full Resources\n",
    "# exp_names = ['21_12_30_feddef_n40_linf0_5_G0_0/','22_01_02_feddef_n40_linf0_5_G0_25_R1_0/',\n",
    "#              '22_01_02_feddef_n40_linf0_5_G0_5_R1_0/', '22_01_02_feddef_n40_linf0_5_G0_75_R1_0/',\n",
    "#              '22_01_02_feddef_n40_linf0_5_G1_0_R1_0/']\n",
    "\n",
    "# Q - adv freq\n",
    "# exp_names = ['22_01_03_feddef_n40_linf0_5_G0_5_R1_0_Q3/', '22_01_03_feddef_n40_linf0_5_G0_5_R1_0_Q5/',\n",
    "#              '22_01_02_feddef_n40_linf0_5_G0_5_R1_0/', '22_01_03_feddef_n40_linf0_5_G0_5_R1_0_Q20/']\n",
    "\n",
    "# Ep - perturbation amount\n",
    "exp_names = ['22_01_04_feddef_n40_linf0_5_G0_5_R1_0_Q10_eps0_1/', '22_01_02_feddef_n40_linf0_5_G0_5_R1_0/',\n",
    "             '22_01_04_feddef_n40_linf0_5_G0_5_R1_0_Q10_eps1/', '22_01_04_feddef_n40_linf0_5_G0_5_R1_0_Q10_eps2/',\n",
    "             '22_01_04_feddef_n40_linf0_5_G0_5_R1_0_Q10_eps4/']\n",
    "\n",
    "base = 'weights/cifar/'\n",
    "train_item = 'train_client_weights.npy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Measurement Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_victims = 8\n",
    "num_exp = len(exp_names)\n",
    "# Set Up Dictionaries -- list holds the adversary idx\n",
    "exp_logs = {}\n",
    "\n",
    "for j in range(num_exp):\n",
    "    logs_adv = []\n",
    "\n",
    "    for i in range(num_victims):\n",
    "        adv_dict = {}\n",
    "        adv_dict['orig_acc_transfers'] = None\n",
    "        adv_dict['orig_similarities'] = None\n",
    "        adv_dict['adv_acc_transfers'] = None\n",
    "        adv_dict['adv_similarities_target'] = None\n",
    "        adv_dict['adv_similarities_untarget'] = None\n",
    "        adv_dict['adv_target'] = None\n",
    "        adv_dict['adv_miss'] = None\n",
    "        adv_dict['metric_alignment'] = None\n",
    "        adv_dict['ib_distance_legit'] = None\n",
    "        adv_dict['ib_distance_adv'] = None\n",
    "        \n",
    "        logs_adv += [adv_dict]\n",
    "    \n",
    "    exp_logs[j] = logs_adv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Each Model and Perform Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing file 22_01_04_feddef_n40_linf0_5_G0_5_R1_0_Q10_eps0_1/ ...\n",
      "\t Adv idx: 0\n",
      "\t Adv idx: 1\n",
      "\t Adv idx: 2\n",
      "\t Adv idx: 3\n",
      "\t Adv idx: 4\n",
      "\t Adv idx: 5\n",
      "\t Adv idx: 6\n",
      "\t Adv idx: 7\n",
      "processing file 22_01_02_feddef_n40_linf0_5_G0_5_R1_0/ ...\n",
      "\t Adv idx: 0\n",
      "\t Adv idx: 1\n",
      "\t Adv idx: 2\n",
      "\t Adv idx: 3\n",
      "\t Adv idx: 4\n",
      "\t Adv idx: 5\n",
      "\t Adv idx: 6\n",
      "\t Adv idx: 7\n",
      "processing file 22_01_04_feddef_n40_linf0_5_G0_5_R1_0_Q10_eps1/ ...\n",
      "\t Adv idx: 0\n",
      "\t Adv idx: 1\n",
      "\t Adv idx: 2\n",
      "\t Adv idx: 3\n",
      "\t Adv idx: 4\n",
      "\t Adv idx: 5\n",
      "\t Adv idx: 6\n",
      "\t Adv idx: 7\n",
      "processing file 22_01_04_feddef_n40_linf0_5_G0_5_R1_0_Q10_eps2/ ...\n",
      "\t Adv idx: 0\n",
      "\t Adv idx: 1\n",
      "\t Adv idx: 2\n",
      "\t Adv idx: 3\n",
      "\t Adv idx: 4\n",
      "\t Adv idx: 5\n",
      "\t Adv idx: 6\n",
      "\t Adv idx: 7\n",
      "processing file 22_01_04_feddef_n40_linf0_5_G0_5_R1_0_Q10_eps4/ ...\n",
      "\t Adv idx: 0\n",
      "\t Adv idx: 1\n",
      "\t Adv idx: 2\n",
      "\t Adv idx: 3\n",
      "\t Adv idx: 4\n",
      "\t Adv idx: 5\n",
      "\t Adv idx: 6\n",
      "\t Adv idx: 7\n"
     ]
    }
   ],
   "source": [
    "# Inter Boundary Distance Metric\n",
    "num_trials = 50\n",
    "batch_size = 5000\n",
    "\n",
    "\n",
    "for j in range(num_exp):\n",
    "    print('processing file', exp_names[j], '...')\n",
    "    \n",
    "    # Change name if need be\n",
    "    args_.save_path = base + exp_names[j]\n",
    "\n",
    "    # Import weights for aggregator\n",
    "    aggregator.load_state(args_.save_path)\n",
    "\n",
    "    # This is where the models are stored -- one for each mixture --> learner.model for nn\n",
    "    hypotheses = aggregator.global_learners_ensemble.learners\n",
    "\n",
    "    # obtain the state dict for each of the weights \n",
    "    weights_h = []\n",
    "\n",
    "    for h in hypotheses:\n",
    "        weights_h += [h.model.state_dict()]\n",
    "        \n",
    "    weight_name = args_.save_path + train_item\n",
    "    weights = np.load(weight_name)\n",
    "    np.set_printoptions(formatter={'float': lambda x: \"{0:0.2f}\".format(x)})\n",
    "\n",
    "    # Set model weights\n",
    "    model_weights = []\n",
    "    num_models = num_victims\n",
    "\n",
    "    for i in range(num_models):\n",
    "        model_weights += [weights[i]]\n",
    "\n",
    "\n",
    "    # Generate the weights to test on as linear combinations of the model_weights\n",
    "    models_test = []\n",
    "\n",
    "    for (w0,w1,w2) in model_weights:\n",
    "        # first make the model with empty weights\n",
    "        new_model = copy.deepcopy(hypotheses[0].model)\n",
    "        new_model.eval()\n",
    "        new_weight_dict = copy.deepcopy(weights_h[0])\n",
    "        for key in weights_h[0]:\n",
    "            new_weight_dict[key] = w0*weights_h[0][key] + w1*weights_h[1][key] + w2*weights_h[2][key]\n",
    "        new_model.load_state_dict(new_weight_dict)\n",
    "        models_test += [new_model]\n",
    "        \n",
    "    # Run Measurements for both targetted and untargeted analysis\n",
    "    victim_idxs = range(num_victims)\n",
    "    \n",
    "    t1 = Transferer(models_list=models_test, dataloader=dataloader)\n",
    "    t1.generate_victims(victim_idxs)\n",
    "    \n",
    "    t2 = Boundary_Transferer(models_list=models_test, dataloader=dataloader)\n",
    "    t2.victim_idx = victim_idxs\n",
    "    \n",
    "    for adv_idx in victim_idxs:\n",
    "        print(\"\\t Adv idx:\", adv_idx)\n",
    "        # Perform Attacks\n",
    "        t1.atk_params = PGD_Params()\n",
    "        t1.atk_params.set_params(batch_size=500, iteration = 10,\n",
    "                       target = 5, x_val_min = torch.min(data_x), x_val_max = torch.max(data_x),\n",
    "                       step_size = 0.05, step_norm = \"inf\", eps = 4.5, eps_norm = 2)\n",
    "        \n",
    "        t1.generate_advNN(adv_idx)\n",
    "        t1.generate_xadv(atk_type = \"pgd\")\n",
    "        t1.send_to_victims(victim_idxs)\n",
    "        t1.check_empirical_metrics(orig_flag = True)\n",
    "\n",
    "        # Log Performance\n",
    "        exp_logs[j][adv_idx]['orig_acc_transfers'] = copy.deepcopy(t1.orig_acc_transfers)\n",
    "        exp_logs[j][adv_idx]['orig_similarities'] = copy.deepcopy(t1.orig_similarities)\n",
    "        exp_logs[j][adv_idx]['adv_acc_transfers'] = copy.deepcopy(t1.adv_acc_transfers)\n",
    "        exp_logs[j][adv_idx]['adv_similarities_target'] = copy.deepcopy(t1.adv_similarities)        \n",
    "        exp_logs[j][adv_idx]['adv_target'] = copy.deepcopy(t1.adv_target_hit)\n",
    "        exp_logs[j][adv_idx]['metric_alignment'] = copy.deepcopy(t1.metric_alignment)\n",
    "        \n",
    "        # Miss attack\n",
    "        t1.atk_params.set_params(batch_size=500, iteration = 10,\n",
    "                       target = -1, x_val_min = torch.min(data_x), x_val_max = torch.max(data_x),\n",
    "                       step_size = 0.05, step_norm = \"inf\", eps = 4.5, eps_norm = 2)\n",
    "        t1.generate_xadv(atk_type = \"pgd\")\n",
    "        t1.send_to_victims(victim_idxs)\n",
    "        exp_logs[j][adv_idx]['adv_miss'] = copy.deepcopy(t1.adv_acc_transfers)\n",
    "        exp_logs[j][adv_idx]['adv_similarities_untarget'] = copy.deepcopy(t1.adv_similarities)\n",
    "        \n",
    "        # Inter-boundary Distance\n",
    "#         t2.base_nn_idx = adv_idx\n",
    "#         t2.atk_params = PGD_Params()\n",
    "#         t2.atk_params.set_params(batch_size=500, iteration = 30,\n",
    "#                        target = -1, x_val_min = torch.min(data_x), x_val_max = torch.max(data_x),\n",
    "#                        step_size = 0.05, step_norm = \"inf\", eps = 5, eps_norm = 2)\n",
    "#         t2.set_adv_NN(t2.base_nn_idx)\n",
    "        \n",
    "#         dists_measure_legit = np.zeros([num_trials, len(t2.victim_idx)])\n",
    "#         dists_measure_adv = np.zeros([num_trials, len(t2.victim_idx)])\n",
    "#         dists_measure_adv_ensemble = np.zeros([num_trials, len(t2.victim_idx)])\n",
    "        \n",
    "#         for t in range(num_trials):\n",
    "#             print('\\t \\t num_trial', t)\n",
    "        \n",
    "#             base_ep_legit, victim_eps_legit = t2.legitimate_direction(batch_size=batch_size, ep_granularity = 0.3, \n",
    "#                                                                   rep_padding = 1000, new_point = True,print_res = False)\n",
    "\n",
    "#             base_ep_adv, victim_eps_adv = t2.adversarial_direction(ep_granularity = 0.3, \n",
    "#                                                                   rep_padding = 1000, new_point = False,print_res = False)\n",
    "\n",
    "#             idx = 0\n",
    "#             for key, value in victim_eps_legit.items():\n",
    "#                 dists_measure_legit[t,idx] = np.abs(base_ep_legit-value)\n",
    "#                 idx+=1\n",
    "\n",
    "#             idx = 0\n",
    "#             for key, value in victim_eps_adv.items():\n",
    "#                 dists_measure_adv[t,idx] = np.abs(base_ep_adv - value)\n",
    "#                 idx+=1\n",
    "        \n",
    "#         exp_logs_targetted[j][adv_idx]['ib_distance_legit'] = copy.deepcopy(dists_measure_legit)\n",
    "#         exp_logs_targetted[j][adv_idx]['ib_distance_adv'] = copy.deepcopy(dists_measure_adv)\n",
    "\n",
    "    del t1, models_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtain Metric Values and Make Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['orig_acc_transfers','orig_similarities','adv_acc_transfers','adv_similarities_target',\n",
    "           'adv_similarities_untarget','adv_target','adv_miss'] #,'metric_alignment']\n",
    "\n",
    "val_keys = ['orig_acc', 'orig_sim', 'adv_acc','adv_sim_target','adv_sim_untarget',\n",
    "            'adv_target','adv_miss'] #,'grad_align']\n",
    "\n",
    "# Set Up Dictionaries -- list holds the adversary idx\n",
    "exp_tables = {}\n",
    "exp_values = {}\n",
    "\n",
    "for j in range(num_exp):\n",
    "    curr_dict = {}\n",
    "    val_dict = {}\n",
    "    \n",
    "    orig_acc = np.zeros([num_victims,num_victims]) \n",
    "    orig_sim = np.zeros([num_victims,num_victims]) \n",
    "    adv_acc = np.zeros([num_victims,num_victims]) \n",
    "    adv_sim_target = np.zeros([num_victims,num_victims]) \n",
    "    adv_sim_untarget = np.zeros([num_victims,num_victims]) \n",
    "    adv_target= np.zeros([num_victims,num_victims]) \n",
    "    adv_miss = np.zeros([num_victims,num_victims]) \n",
    "    grad_align = np.zeros([num_victims,num_victims]) \n",
    "\n",
    "    for adv_idx in range(len(victim_idxs)):\n",
    "        for victim in range(len(victim_idxs)):\n",
    "            orig_acc[adv_idx,victim] = exp_logs[j][victim_idxs[adv_idx]][metrics[0]][victim_idxs[victim]].data.tolist()\n",
    "            orig_sim[adv_idx,victim] = exp_logs[j][victim_idxs[adv_idx]][metrics[1]][victim_idxs[victim]].data.tolist()\n",
    "            adv_acc[adv_idx,victim] = exp_logs[j][victim_idxs[adv_idx]][metrics[2]][victim_idxs[victim]].data.tolist()\n",
    "            adv_sim_target[adv_idx,victim] = exp_logs[j][victim_idxs[adv_idx]][metrics[3]][victim_idxs[victim]].data.tolist()\n",
    "            adv_sim_untarget[adv_idx,victim] = exp_logs[j][victim_idxs[adv_idx]][metrics[4]][victim_idxs[victim]].data.tolist()\n",
    "            adv_target[adv_idx,victim] = exp_logs[j][victim_idxs[adv_idx]][metrics[5]][victim_idxs[victim]].data.tolist()\n",
    "            adv_miss[adv_idx,victim] = exp_logs[j][victim_idxs[adv_idx]][metrics[6]][victim_idxs[victim]].data.tolist()\n",
    "#             grad_align[adv_idx,victim] = exp_logs[j][adv_idx][metrics[7]][victim].data.tolist()\n",
    "            \n",
    "    curr_dict['orig_acc'] = copy.deepcopy(orig_acc) \n",
    "    curr_dict['orig_sim'] = copy.deepcopy(orig_sim) \n",
    "    curr_dict['adv_acc'] = copy.deepcopy(adv_acc) \n",
    "    curr_dict['adv_sim_target'] = copy.deepcopy(adv_sim_target)\n",
    "    curr_dict['adv_sim_untarget'] = copy.deepcopy(adv_sim_untarget)\n",
    "    curr_dict['adv_target'] = copy.deepcopy(adv_target)\n",
    "    curr_dict['adv_miss'] = copy.deepcopy(adv_miss)\n",
    "#     curr_dict['grad_align'] = copy.deepcopy(grad_align)\n",
    "    \n",
    "    val_dict['orig_acc'] = np.mean(orig_acc)\n",
    "    val_dict['orig_sim'] = avg_nondiag(orig_sim) \n",
    "    val_dict['adv_acc'] = np.mean(adv_acc) \n",
    "    val_dict['adv_sim_target'] = avg_nondiag(adv_sim_target)\n",
    "    val_dict['adv_sim_untarget'] = avg_nondiag(adv_sim_untarget)\n",
    "    val_dict['adv_target'] = avg_nondiag(adv_target)\n",
    "    val_dict['adv_miss'] = avg_nondiag(adv_miss)\n",
    "#     val_dict['grad_align'] = avg_nondiag(grad_align)\n",
    "    \n",
    "    exp_tables[j] = copy.deepcopy(curr_dict)\n",
    "    exp_values[j] = copy.deepcopy(val_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_store = np.zeros([len(val_keys), len(exp_names)])\n",
    "for j in range(len(exp_names)):\n",
    "    for m in range(len(val_keys)):\n",
    "        data_store[m,j] = exp_values[j][val_keys[m]]\n",
    "\n",
    "# row_items = ['G0.0','G0.25','G0.5','G0.75','G1.0']\n",
    "# row_items = ['R0.1','R0.2','R0.4','R0.5','R0.6']\n",
    "# row_items = ['Q3', 'Q5', 'Q10', 'Q20']\n",
    "row_items = ['ep0.1', 'ep0.5', 'ep1.0', 'ep2.0','ep4.0']\n",
    "df = pd.DataFrame(data_store, columns = row_items, index = val_keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ep0.1</th>\n",
       "      <th>ep0.5</th>\n",
       "      <th>ep1.0</th>\n",
       "      <th>ep2.0</th>\n",
       "      <th>ep4.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>orig_acc</th>\n",
       "      <td>0.719375</td>\n",
       "      <td>0.699500</td>\n",
       "      <td>0.723344</td>\n",
       "      <td>0.781813</td>\n",
       "      <td>0.765813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orig_sim</th>\n",
       "      <td>0.681321</td>\n",
       "      <td>0.687143</td>\n",
       "      <td>0.735821</td>\n",
       "      <td>0.792036</td>\n",
       "      <td>0.760143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adv_acc</th>\n",
       "      <td>0.666969</td>\n",
       "      <td>0.571156</td>\n",
       "      <td>0.536406</td>\n",
       "      <td>0.448031</td>\n",
       "      <td>0.436406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adv_sim_target</th>\n",
       "      <td>0.631214</td>\n",
       "      <td>0.588857</td>\n",
       "      <td>0.663821</td>\n",
       "      <td>0.727750</td>\n",
       "      <td>0.685857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adv_sim_untarget</th>\n",
       "      <td>0.472643</td>\n",
       "      <td>0.453536</td>\n",
       "      <td>0.522464</td>\n",
       "      <td>0.601643</td>\n",
       "      <td>0.557071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adv_target</th>\n",
       "      <td>0.172821</td>\n",
       "      <td>0.248500</td>\n",
       "      <td>0.338643</td>\n",
       "      <td>0.458214</td>\n",
       "      <td>0.461607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adv_miss</th>\n",
       "      <td>0.533679</td>\n",
       "      <td>0.405964</td>\n",
       "      <td>0.308321</td>\n",
       "      <td>0.174179</td>\n",
       "      <td>0.114857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     ep0.1     ep0.5     ep1.0     ep2.0     ep4.0\n",
       "orig_acc          0.719375  0.699500  0.723344  0.781813  0.765813\n",
       "orig_sim          0.681321  0.687143  0.735821  0.792036  0.760143\n",
       "adv_acc           0.666969  0.571156  0.536406  0.448031  0.436406\n",
       "adv_sim_target    0.631214  0.588857  0.663821  0.727750  0.685857\n",
       "adv_sim_untarget  0.472643  0.453536  0.522464  0.601643  0.557071\n",
       "adv_target        0.172821  0.248500  0.338643  0.458214  0.461607\n",
       "adv_miss          0.533679  0.405964  0.308321  0.174179  0.114857"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FedEM_env",
   "language": "python",
   "name": "fedem_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
