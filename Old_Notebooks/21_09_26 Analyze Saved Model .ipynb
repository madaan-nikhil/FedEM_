{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Saved Model\n",
    "\n",
    "9.27.21\n",
    "\n",
    "#### Summary:\n",
    "- Train (load) a model that was developed by using FedEM\n",
    "- Check the nn class within the model\n",
    "- Figure out how to work Dataloader\n",
    "- Figure out differences between client, test, ensemble_learner -- hypothesis vs clients, linear weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/FedEM\n"
     ]
    }
   ],
   "source": [
    "cd /home/ubuntu/FedEM/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Relevant Libraries\n",
    "Take it from the run_experiment.py folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import General Libraries\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import copy\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import FedEM based Libraries\n",
    "from utils.utils import *\n",
    "from utils.constants import *\n",
    "from utils.args import *\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from run_experiment import *\n",
    "from models import *\n",
    "\n",
    "# Import Transfer Attack\n",
    "from transfer_attacks.Personalized_NN import *\n",
    "from transfer_attacks.Params import *\n",
    "from transfer_attacks.Transferer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Aggregator Pre-requisite\n",
    "- Clients, Test Clients, Ensemble_Learner\n",
    "- Follow through the code in run_experiment.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments used for training\n",
    "class args:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.experiment = \"cifar10\"\n",
    "        self.method = \"FedEM\"\n",
    "        self.decentralized = False\n",
    "        self.sampling_rate = 1.0\n",
    "        self.input_dimension = None\n",
    "        self.output_dimension = None\n",
    "        self.n_learners= 3\n",
    "        self.n_rounds = 10\n",
    "        self.bz = 128\n",
    "        self.local_steps = 1\n",
    "        self.lr_lambda = 0\n",
    "        self.lr =0.03\n",
    "        self.lr_scheduler = 'multi_step'\n",
    "        self.log_freq = 10\n",
    "        self.device = 'cuda'\n",
    "        self.optimizer = 'sgd'\n",
    "        self.mu = 0\n",
    "        self.communication_probability = 0.1\n",
    "        self.q = 1\n",
    "        self.locally_tune_clients = False\n",
    "        self.seed = 1234\n",
    "        self.verbose = 1\n",
    "        self.save_path = 'weights/cifar/21_09_28_first_transfers/'\n",
    "        self.validation = False\n",
    "        \n",
    "    \n",
    "    def __iter__(self):\n",
    "        for attr in dir(self):\n",
    "            if not attr.startswith(\"__\"):\n",
    "                yield attr\n",
    "\n",
    "args_ = args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Saved Aggregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00 0.35 0.65]\n",
      " [0.41 0.59 0.00]\n",
      " [0.42 0.09 0.49]\n",
      " [0.96 0.04 0.00]\n",
      " [0.62 0.00 0.38]\n",
      " [0.00 0.53 0.47]\n",
      " [0.04 0.44 0.52]\n",
      " [0.00 0.20 0.80]\n",
      " [0.00 0.73 0.27]\n",
      " [0.57 0.00 0.43]\n",
      " [0.15 0.00 0.85]\n",
      " [0.00 0.50 0.50]\n",
      " [0.64 0.11 0.25]\n",
      " [0.70 0.00 0.30]\n",
      " [0.31 0.00 0.69]\n",
      " [0.00 0.00 1.00]\n",
      " [0.01 0.89 0.10]\n",
      " [0.78 0.00 0.22]\n",
      " [1.00 0.00 0.00]\n",
      " [1.00 0.00 0.00]\n",
      " [0.71 0.00 0.29]\n",
      " [0.09 0.00 0.91]\n",
      " [0.00 0.15 0.85]\n",
      " [1.00 0.00 0.00]\n",
      " [0.71 0.00 0.29]\n",
      " [0.00 0.16 0.84]\n",
      " [0.00 0.00 1.00]\n",
      " [0.96 0.04 0.00]\n",
      " [0.77 0.00 0.23]\n",
      " [0.81 0.00 0.19]\n",
      " [0.00 0.62 0.38]\n",
      " [0.98 0.02 0.00]\n",
      " [0.68 0.00 0.32]\n",
      " [0.87 0.10 0.03]\n",
      " [0.00 0.36 0.64]\n",
      " [0.48 0.46 0.06]\n",
      " [1.00 0.00 0.00]\n",
      " [0.00 0.01 0.99]\n",
      " [0.00 1.00 0.00]\n",
      " [0.05 0.95 0.00]\n",
      " [0.68 0.32 0.00]\n",
      " [0.01 0.20 0.79]\n",
      " [0.86 0.14 0.00]\n",
      " [0.08 0.00 0.92]\n",
      " [0.00 1.00 0.00]\n",
      " [0.00 0.42 0.58]\n",
      " [0.03 0.00 0.97]\n",
      " [0.98 0.02 0.00]\n",
      " [0.01 0.00 0.99]\n",
      " [0.98 0.00 0.02]\n",
      " [0.00 0.00 1.00]\n",
      " [0.24 0.00 0.76]\n",
      " [0.00 0.00 1.00]\n",
      " [0.99 0.00 0.01]\n",
      " [0.80 0.00 0.20]\n",
      " [0.00 0.86 0.14]\n",
      " [0.00 0.27 0.73]\n",
      " [0.98 0.00 0.02]\n",
      " [0.92 0.00 0.08]\n",
      " [0.89 0.00 0.11]\n",
      " [1.00 0.00 0.00]\n",
      " [0.01 0.09 0.90]\n",
      " [0.33 0.22 0.45]\n",
      " [0.97 0.00 0.03]\n",
      " [0.09 0.89 0.02]\n",
      " [0.99 0.00 0.01]\n",
      " [0.25 0.72 0.03]\n",
      " [0.01 0.98 0.01]\n",
      " [0.02 0.00 0.98]\n",
      " [0.96 0.04 0.00]\n",
      " [0.00 1.00 0.00]\n",
      " [0.93 0.00 0.07]\n",
      " [0.00 0.01 0.99]\n",
      " [0.00 0.00 1.00]\n",
      " [0.98 0.00 0.02]\n",
      " [0.18 0.00 0.82]\n",
      " [0.57 0.00 0.43]\n",
      " [0.02 0.00 0.98]\n",
      " [0.49 0.00 0.50]\n",
      " [0.02 0.00 0.98]]\n"
     ]
    }
   ],
   "source": [
    "weights = np.load(\"weights/cifar/21_09_28_first_transfers/train_client_weights.npy\")\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.2f}\".format(x)})\n",
    "\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy of Run Experiments\n",
    "\n",
    "Try and set up an aggregator without going through the training process to observe the dimensionality of clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Clients initialization..\n",
      "===> Building data iterators..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 80/80 [00:00<00:00, 161.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Initializing clients..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 80/80 [00:34<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Test Clients initialization..\n",
      "===> Building data iterators..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Initializing clients..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++\n",
      "Global..\n",
      "Train Loss: 2.292 | Train Acc: 12.159% |Test Loss: 2.292 | Test Acc: 12.248% |\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "################################################################################\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(args_.seed)\n",
    "\n",
    "data_dir = get_data_dir(args_.experiment)\n",
    "\n",
    "if \"logs_root\" in args_:\n",
    "    logs_root = args_.logs_root\n",
    "else:\n",
    "    logs_root = os.path.join(\"logs\", args_to_string(args_))\n",
    "\n",
    "print(\"==> Clients initialization..\")\n",
    "clients = init_clients(\n",
    "    args_,\n",
    "    root_path=os.path.join(data_dir, \"train\"),\n",
    "    logs_root=os.path.join(logs_root, \"train\")\n",
    ")\n",
    "\n",
    "print(\"==> Test Clients initialization..\")\n",
    "test_clients = init_clients(\n",
    "    args_,\n",
    "    root_path=os.path.join(data_dir, \"test\"),\n",
    "    logs_root=os.path.join(logs_root, \"test\")\n",
    ")\n",
    "\n",
    "logs_path = os.path.join(logs_root, \"train\", \"global\")\n",
    "os.makedirs(logs_path, exist_ok=True)\n",
    "global_train_logger = SummaryWriter(logs_path)\n",
    "\n",
    "logs_path = os.path.join(logs_root, \"test\", \"global\")\n",
    "os.makedirs(logs_path, exist_ok=True)\n",
    "global_test_logger = SummaryWriter(logs_path)\n",
    "\n",
    "global_learners_ensemble = \\\n",
    "    get_learners_ensemble(\n",
    "        n_learners=args_.n_learners,\n",
    "        name=args_.experiment,\n",
    "        device=args_.device,\n",
    "        optimizer_name=args_.optimizer,\n",
    "        scheduler_name=args_.lr_scheduler,\n",
    "        initial_lr=args_.lr,\n",
    "        input_dim=args_.input_dimension,\n",
    "        output_dim=args_.output_dimension,\n",
    "        n_rounds=args_.n_rounds,\n",
    "        seed=args_.seed,\n",
    "        mu=args_.mu\n",
    ")\n",
    "\n",
    "\n",
    "if args_.decentralized:\n",
    "    aggregator_type = 'decentralized'\n",
    "else:\n",
    "    aggregator_type = AGGREGATOR_TYPE[args_.method]\n",
    "\n",
    "aggregator =\\\n",
    "    get_aggregator(\n",
    "        aggregator_type=aggregator_type,\n",
    "        clients=clients,\n",
    "        global_learners_ensemble=global_learners_ensemble,\n",
    "        lr_lambda=args_.lr_lambda,\n",
    "        lr=args_.lr,\n",
    "        q=args_.q,\n",
    "        mu=args_.mu,\n",
    "        communication_probability=args_.communication_probability,\n",
    "        sampling_rate=args_.sampling_rate,\n",
    "        log_freq=args_.log_freq,\n",
    "        global_train_logger=global_train_logger,\n",
    "        global_test_logger=global_test_logger,\n",
    "        test_clients=test_clients,\n",
    "        verbose=args_.verbose,\n",
    "        seed=args_.seed\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Combination of Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import weights for aggregator\n",
    "aggregator.load_state(args_.save_path)\n",
    "\n",
    "# This is where the models are stored -- one for each mixture --> learner.model for nn\n",
    "hypotheses = aggregator.global_learners_ensemble.learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the state dict for each of the weights \n",
    "weights_h = []\n",
    "\n",
    "for h in hypotheses:\n",
    "    weights_h += [h.model.state_dict()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine here the different weights of models we desire to compare\n",
    "model_weights = []\n",
    "\n",
    "model_weights += [(1,0,0),(0,1,0),(0,0,1)] #single mixture\n",
    "model_weights += [(0.5,0.5,0),(0.5,0,0.5),(0,0.5,0.5)] # half of 2 mixtures of 3\n",
    "model_weights += [(1/3,1/3,1/3)] # Equally balanced from 3 mixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the weights to test on as linear combinations of the model_weights\n",
    "models_test = []\n",
    "\n",
    "for (w0,w1,w2) in model_weights:\n",
    "    # first make the model with empty weights\n",
    "    new_model = copy.deepcopy(hypotheses[0].model)\n",
    "    new_model.eval()\n",
    "    new_weight_dict = copy.deepcopy(weights_h[0])\n",
    "    for key in weights_h[0]:\n",
    "        new_weight_dict[key] = w0*weights_h[0][key] + w1*weights_h[1][key] + w2*weights_h[2][key]\n",
    "    new_model.load_state_dict(new_weight_dict)\n",
    "    models_test += [new_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of Each of the Neural Networks\n",
    "- Figure out how to bring the dataloader and load points\n",
    "- Check accuracy for different combinations of hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Validation Data across all clients as test\n",
    "data_x = []\n",
    "data_y = []\n",
    "\n",
    "for i in range(len(clients)):\n",
    "    daniloader = clients[i].val_iterator\n",
    "    for (x,y,idx) in daniloader.dataset:\n",
    "        data_x.append(x)\n",
    "        data_y.append(y)\n",
    "\n",
    "data_x = torch.stack(data_x)\n",
    "data_y = torch.stack(data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1, 0, 0): tensor(0.8065, device='cuda:0'),\n",
       " (0, 1, 0): tensor(0.7225, device='cuda:0'),\n",
       " (0, 0, 1): tensor(0.8505, device='cuda:0'),\n",
       " (0.5, 0.5, 0): tensor(0.7970, device='cuda:0'),\n",
       " (0.5, 0, 0.5): tensor(0.8555, device='cuda:0'),\n",
       " (0, 0.5, 0.5): tensor(0.7910, device='cuda:0'),\n",
       " (0.3333333333333333,\n",
       "  0.3333333333333333,\n",
       "  0.3333333333333333): tensor(0.8300, device='cuda:0')}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "# Pass subset of dataset through (10000 samples) and check accuracy on each model\n",
    "bz = 2000 # < 47000\n",
    "acc_dict = {}\n",
    "\n",
    "# Select Random Subset of values\n",
    "samples = random.sample(range(data_y.shape[0]),bz)\n",
    "\n",
    "x_data = data_x[samples].to(device='cuda')\n",
    "y_data = data_y[samples].to(device='cuda')\n",
    "\n",
    "# Obtain Accuracy\n",
    "for i in range(len(model_weights)):\n",
    "    preds = models_test[i](x_data)\n",
    "    preds_category = torch.argmax(preds,dim=1)\n",
    "    acc_dict[model_weights[i]] = (preds_category == y_data).float().sum()/bz\n",
    "\n",
    "acc_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Evasion Attacks on Each of the Models\n",
    "- Import victim NN and make modifications to work with the CIFAR model (any model)\n",
    "- Make it work with any dataset that is given to it (tensor or dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a Custom Class for Dataloader that has flexible batch size\n",
    "class Custom_Dataloader:\n",
    "    def __init__(self, x_data, y_data):\n",
    "        self.x_data = x_data # Tensor + cuda\n",
    "        self.y_data = y_data # Tensor + cuda\n",
    "        \n",
    "    def load_batch(self,batch_size,mode = 'test'):\n",
    "        samples = random.sample(range(self.y_data.shape[0]),batch_size)\n",
    "        out_x_data = self.x_data[samples].to(device='cuda')\n",
    "        out_y_data = self.y_data[samples].to(device='cuda')\n",
    "        \n",
    "        return out_x_data, out_y_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader from validation dataset that allows for diverse batch size\n",
    "dataloader = Custom_Dataloader(data_x, data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create adversary neural network from one of the test models\n",
    "adv_nn = Adv_NN(trained_network=models_test[0], dataloader=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate attack parameters\n",
    "atk_params = IFSGM_Params()\n",
    "atk_params.target = 5\n",
    "atk_params.x_val_min = torch.min(x_data).item()\n",
    "atk_params.x_val_max = torch.max(x_data).item()\n",
    "atk_params.batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- FGSM Batch Size: 10 ----\n",
      "\n",
      "Orig Target: [8, 2, 4, 6, 5, 3, 2, 9, 8, 2]\n",
      "Orig Output: [8, 2, 4, 6, 3, 3, 2, 9, 8, 2]\n",
      "ADV Output : [8, 6, 5, 5, 5, 5, 5, 5, 8, 5] \n",
      "\n",
      "Orig Loss  : -9.537945747375488\n",
      "ADV Loss   : -4.195797443389893 \n",
      "\n",
      "Orig Acc   : 0.9000000357627869\n",
      "ADV Acc    : 0.30000001192092896\n"
     ]
    }
   ],
   "source": [
    "# Perform attack on itself - IFGSM\n",
    "adv_nn.i_fgsm(atk_params, print_info=True, mode = 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Attack Single\n",
    "- Utilize the transferer class across all of the different models and perform a sweep from different settings on how much attacks are transferable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make transferer and Assign model index\n",
    "adv_idx = 0\n",
    "victim_idxs = [0,1,2,3,4,5,6]\n",
    "\n",
    "t1 = Transferer(models_list=models_test, dataloader=dataloader)\n",
    "t1.generate_advNN(adv_idx)\n",
    "t1.generate_victims(victim_idxs)\n",
    "\n",
    "t1.generate_xadv(atk_type = \"ifsgm\")\n",
    "t1.send_to_victims(victim_idxs)\n",
    "t1.check_empirical_metrics(orig_flag = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: tensor(0.8000, device='cuda:0'),\n",
       " 1: tensor(0.7200, device='cuda:0'),\n",
       " 2: tensor(0.8600, device='cuda:0'),\n",
       " 3: tensor(0.8100, device='cuda:0'),\n",
       " 4: tensor(0.8500, device='cuda:0'),\n",
       " 5: tensor(0.7800, device='cuda:0'),\n",
       " 6: tensor(0.8000, device='cuda:0')}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.orig_acc_transfers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: tensor(1., device='cuda:0'),\n",
       " 1: tensor(0.6300, device='cuda:0'),\n",
       " 2: tensor(0.7100, device='cuda:0'),\n",
       " 3: tensor(0.7500, device='cuda:0'),\n",
       " 4: tensor(0.7800, device='cuda:0'),\n",
       " 5: tensor(0.6500, device='cuda:0'),\n",
       " 6: tensor(0.7100, device='cuda:0')}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.orig_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: tensor(0.2600, device='cuda:0'),\n",
       " 1: tensor(0.6300, device='cuda:0'),\n",
       " 2: tensor(0.6300, device='cuda:0'),\n",
       " 3: tensor(0.4800, device='cuda:0'),\n",
       " 4: tensor(0.4900, device='cuda:0'),\n",
       " 5: tensor(0.5900, device='cuda:0'),\n",
       " 6: tensor(0.5100, device='cuda:0')}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.adv_acc_transfers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: tensor(1., device='cuda:0'),\n",
       " 1: tensor(0.3100, device='cuda:0'),\n",
       " 2: tensor(0.5600, device='cuda:0'),\n",
       " 3: tensor(0.6400, device='cuda:0'),\n",
       " 4: tensor(0.7200, device='cuda:0'),\n",
       " 5: tensor(0.5000, device='cuda:0'),\n",
       " 6: tensor(0.7000, device='cuda:0')}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.adv_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: tensor(0.0300, device='cuda:0'),\n",
       " 1: tensor(0.0500, device='cuda:0'),\n",
       " 2: tensor(0.1200, device='cuda:0'),\n",
       " 3: tensor(0.0600, device='cuda:0'),\n",
       " 4: tensor(0.0900, device='cuda:0'),\n",
       " 5: tensor(0.1100, device='cuda:0'),\n",
       " 6: tensor(0.1100, device='cuda:0')}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.orig_target_hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: tensor(0.7300, device='cuda:0'),\n",
       " 1: tensor(0.1800, device='cuda:0'),\n",
       " 2: tensor(0.3900, device='cuda:0'),\n",
       " 3: tensor(0.4600, device='cuda:0'),\n",
       " 4: tensor(0.5300, device='cuda:0'),\n",
       " 5: tensor(0.3600, device='cuda:0'),\n",
       " 6: tensor(0.4900, device='cuda:0')}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.adv_target_hit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Empirical Metrics\n",
    "- Size of input gradient - across data distribution across all victim NN\n",
    "- Gradient Alignment - Between the surrogate and each of the victim NN\n",
    "- Variance of loss - Just for the surrogate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1.check_empirical_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.5668, device='cuda:0', grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.metric_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: tensor(0.),\n",
       " 1: tensor(1.4098),\n",
       " 2: tensor(1.3749),\n",
       " 3: tensor(1.1213),\n",
       " 4: tensor(1.1321),\n",
       " 5: tensor(1.3215),\n",
       " 6: tensor(1.1811)}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.metric_alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: tensor(0.0710),\n",
       " 1: tensor(0.2525),\n",
       " 2: tensor(0.1249),\n",
       " 3: tensor(0.0486),\n",
       " 4: tensor(0.0417),\n",
       " 5: tensor(0.0477),\n",
       " 6: tensor(0.0296)}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.metric_ingrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FedEM_env",
   "language": "python",
   "name": "fedem_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
